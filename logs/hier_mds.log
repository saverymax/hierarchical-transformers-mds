DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/facebook/bart-large/config.json HTTP/1.1" 200 0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json from cache at /lscratch/60844499/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.40bd49bcec9d93d8b0bfbd020088e2e1b6e6bb03e8e80aa5144638f90ca6bd61
INFO:transformers.configuration_utils:Model config BartConfig {
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForMaskedLM",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "normalize_before": false,
  "normalize_embedding": true,
  "num_hidden_layers": 12,
  "output_past": false,
  "pad_token_id": 1,
  "prefix": " ",
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "no_repeat_ngram_size": 3,
      "num_beams": 4
    }
  },
  "vocab_size": 50265
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-large-vocab.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-large-merges.txt HTTP/1.1" 200 0
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /lscratch/60844499/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /lscratch/60844499/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
INFO:root:EOS token from facebook/bart-large tokenizer </s>, 2
INFO:root:BOS token <s>, 0
INFO:root:Hierarchical config: HierarchicalTransformerConfig {
  "batch_size": 3,
  "bos_token_id": 2,
  "dec_hidden_dim": 128,
  "decoder_attn_mask": true,
  "decoder_layers": 2,
  "decoder_type": "transformer",
  "dropout": 0.1,
  "enc_hidden_dim": 128,
  "eos_token": "</s>",
  "eos_token_id": 2,
  "ffw_dim": 1024,
  "global_enc_layers": 2,
  "hf_model": null,
  "init_bert_weights": false,
  "is_encoder_decoder": true,
  "k_docs": null,
  "local_enc_layers": 2,
  "max_docs": 5,
  "max_seq_len": 256,
  "mmr": true,
  "model_type": "hier_transformer",
  "multi_head_pooling": true,
  "n_att_heads": 4,
  "padding_mask": true,
  "query_doc_attn": false,
  "use_cls_token": false,
  "vocab_size": 50265
}

INFO:root:Initializing random embeddings
INFO:root:Using cuda:0
INFO:root:Training tasks provided: ['ebm']
INFO:root:Number of all articles: 2074
INFO:root:Validation tasks provided: ['ebm']
INFO:root:Number of all articles: 311
INFO:root:Using label smoothing loss of 0.1
INFO:root:Beginning training for 1 epochs, batch size 3
INFO:root:Encoded query: tensor([41552,   565,  2336,   530, 15698,   381, 13386, 28696, 46392,  7744,
        15698,  6834,  2196,    32,   144,  2375,    13,  7212,     7,  3814],
       device='cuda:0')
INFO:root:Decoded query: <TASK> EBM <QUESTION> Which drugs are most effective for moderate to severe
INFO:root:Encoded source: tensor([ 3972, 10516,     5,  1078,     9,  6626, 11104, 18017,  1168,  6909,
           36,   597,  1864,    43,    11,  6676,    19,    97, 16674,    11],
       device='cuda:0')
INFO:root:Decoded source: To evaluate the safety of fluoroquinolones (FQ) in comparison with other antibiotics in
INFO:root:Encoded decoder input:
tensor([    2, 37666,   557,    16,   956,     7,  1100,     5,   251,    12,
         1279, 22081,     9,  6626,  4325,   594,   833,  1416,     4,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1], device='cuda:0')
INFO:root:Decoded shifted summary:
</s>Further research is needed to address the long-term efficacy of fluoxetine treatment.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
INFO:root:doc_mask size torch.Size([3, 5])
INFO:root:doc_mask tensor([[True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True]], device='cuda:0')
INFO:root:TOken mask! torch.Size([3, 5, 256])
INFO:root:TOken mask! tensor([[[ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False],
         [ True,  True,  True,  ..., False, False, False]]], device='cuda:0')
INFO:root:target mask! torch.Size([3, 256])
INFO:root:query mask! torch.Size([3, 256])
DEBUG:root:token emb size for input: torch.Size([3, 5, 256, 128])
DEBUG:root:tgt emb size: torch.Size([256, 3, 128])
DEBUG:root:query emb size: torch.Size([256, 3, 128])
DEBUG:root:tok emb size for input after initial reshape: torch.Size([256, 15, 128])
DEBUG:root:token mask emb size for input after initial reshape: torch.Size([15, 256])
INFO:root:initial tok emb tensor([[[ 7.0532e-01, -8.8265e-01, -1.0381e-01,  ...,  6.8977e-01,
          -9.1937e-01,  3.3893e-01],
         [ 4.4229e-01, -2.2788e-01,  4.3221e-01,  ..., -5.7483e-01,
          -9.2620e-01, -3.7312e-02],
         [ 9.0284e-01,  9.6831e-01, -7.0042e-01,  ...,  1.1692e-01,
          -9.7632e-01, -1.9126e-01],
         ...,
         [ 1.0601e+00, -8.6678e-01, -1.9785e-01,  ...,  1.5332e-01,
           9.7132e-01,  1.9791e-01],
         [-3.7806e-01, -2.6191e-01, -8.5421e-01,  ..., -6.4134e-01,
          -6.4842e-01,  2.7204e-01],
         [ 3.6286e-01, -9.3757e-01,  9.0026e-01,  ...,  1.9294e-01,
           1.0882e+00,  5.1568e-01]],

        [[ 2.7545e-01,  1.4646e-02,  1.0822e+00,  ...,  1.0473e+00,
           5.3700e-01,  8.4522e-01],
         [-4.4158e-01, -9.1007e-01, -9.0020e-01,  ..., -8.2906e-01,
          -8.8906e-01, -4.3117e-01],
         [-2.1077e-01, -1.1063e+00,  5.1393e-01,  ...,  1.8619e-01,
           1.0553e+00,  6.6060e-01],
         ...,
         [ 1.0197e+00, -4.8175e-01, -9.2268e-04,  ..., -9.4472e-01,
          -4.3705e-01, -3.5477e-01],
         [-8.7682e-01,  5.9020e-01, -4.7408e-01,  ...,  5.8062e-01,
          -1.0704e+00,  4.5333e-01],
         [-8.6331e-01,  7.3958e-01, -1.0571e+00,  ..., -9.1832e-01,
           8.5189e-01, -4.2315e-02]],

        [[ 3.2308e-02, -1.4772e-01,  7.6556e-01,  ..., -7.0461e-01,
           5.2632e-01,  6.6456e-01],
         [-1.6843e-01,  2.9952e-01, -1.0677e+00,  ...,  5.7893e-01,
           4.2594e-01,  4.0438e-02],
         [ 7.8703e-01, -2.8176e-01, -1.0869e+00,  ..., -3.2423e-02,
           7.1616e-01,  4.9283e-02],
         ...,
         [ 3.0923e-01, -1.8195e-02,  2.2455e-01,  ...,  7.0135e-01,
           4.8589e-01,  9.6316e-01],
         [ 7.5858e-01, -6.4695e-01,  5.8838e-01,  ...,  9.6664e-01,
           1.7988e-01, -9.8534e-01],
         [ 2.9178e-01,  4.1630e-01,  9.1217e-01,  ...,  4.6523e-01,
           4.0417e-01, -6.7342e-01]],

        ...,

        [[-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         ...,
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01]],

        [[-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         ...,
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01]],

        [[-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         ...,
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01],
         [-7.5043e-01, -1.6359e-01, -5.1944e-02,  ...,  6.0383e-01,
           3.4409e-01, -2.9488e-01]]], device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:src mask tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
DEBUG:root:layer type: local
INFO:root:local layer emb: tensor([[[ 0.2589, -0.0528, -1.3663,  ...,  2.7817, -1.5012,  0.8414],
         [-0.8438,  0.0537, -1.2327,  ...,  0.1199, -2.1162,  1.1981],
         [ 0.5108,  1.5178, -2.0813,  ...,  1.9069, -1.4145, -0.5668],
         ...,
         [ 0.8101,  0.0238, -0.6053,  ...,  2.0544,  0.8680,  2.1614],
         [-0.4096,  1.0482, -1.4425,  ...,  1.1672, -0.5692,  1.5194],
         [-0.0253,  0.0369, -0.0603,  ...,  1.4863,  0.6179,  1.3138]],

        [[ 0.0481, -0.0059,  1.3714,  ...,  2.0192, -0.1682,  2.2117],
         [-0.0953, -0.4966, -0.5360,  ...,  0.3987, -1.3857,  0.4537],
         [ 0.3287,  0.5718,  0.4394,  ...,  1.4992,  1.0485,  1.5313],
         ...,
         [ 1.2738, -0.1531,  0.0572,  ..., -0.4326, -0.6542,  1.1317],
         [-0.6633,  1.3100, -0.7335,  ...,  2.5477, -1.4627,  1.2341],
         [-0.1837,  1.2708, -1.4987,  ...,  0.3473,  0.7090, -0.3562]],

        [[ 0.5224, -0.7277,  1.0824,  ...,  0.5845, -0.1031,  1.9104],
         [ 0.2294, -0.5360, -1.1531,  ...,  1.8841, -0.2558,  0.7341],
         [ 1.4047, -0.9730, -1.0552,  ...,  1.3335,  0.9415,  0.9369],
         ...,
         [-0.6614, -0.9995,  0.8481,  ...,  1.6161, -0.2282,  2.1447],
         [-0.0342, -1.1429,  0.5810,  ...,  2.8291, -0.5274,  0.1713],
         [ 0.2126, -0.1122,  0.7072,  ...,  1.7862, -0.1233, -0.2301]],

        ...,

        [[-0.8488,  0.0278, -1.3867,  ...,  1.6543,  0.1000,  1.0783],
         [-0.7906, -0.3282, -1.7912,  ...,  0.9759,  0.0615,  1.0441],
         [-0.8393, -0.2167, -1.8490,  ...,  1.4027, -0.0242,  1.3311],
         ...,
         [-0.5508, -0.0167, -1.1446,  ...,  1.3620,  0.1369, -0.0946],
         [-0.7970, -0.6210, -1.8760,  ...,  1.7559, -0.1019,  1.1376],
         [-0.5255, -0.1512, -2.0755,  ...,  1.3972,  0.4904,  1.2905]],

        [[-1.1516, -1.2201, -0.8882,  ...,  1.9287, -0.0687,  1.0847],
         [-1.1392, -1.1468, -1.4747,  ...,  1.0676, -0.5306,  1.3754],
         [-0.8695, -0.9270, -1.1077,  ...,  1.8261,  0.2654,  0.9821],
         ...,
         [-1.0596, -1.1919, -0.5265,  ...,  1.5428,  0.1872,  1.2142],
         [-0.9778, -1.1321, -1.1192,  ...,  1.7618, -0.3586,  0.5166],
         [-0.9906, -0.2917, -0.9196,  ...,  1.6775,  0.3121,  1.2158]],

        [[-1.9499, -0.7749,  0.0190,  ...,  1.5704, -0.1017,  1.0198],
         [-0.6509, -0.0427, -0.2077,  ...,  0.2975, -0.2690,  1.2220],
         [-1.6719, -1.2111, -0.2736,  ...,  0.6374, -0.7388,  0.8894],
         ...,
         [-0.7452, -0.9722,  0.1082,  ...,  1.6850,  0.2262,  1.0479],
         [-1.7484, -1.0612,  0.1421,  ...,  1.7509,  0.2799,  0.9241],
         [-2.0773,  0.1482, -0.2907,  ...,  1.0302, -0.3271,  0.7171]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([256, 15, 128]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([256, 15, 128])
INFO:root:query size torch.Size([256, 3, 128])
INFO:root:src padding_mask size torch.Size([15, 256])
INFO:root:Computing multi-head pooling
INFO:root:tensor([[[ 0.2589, -0.0528, -1.3663,  ...,  2.7817, -1.5012,  0.8414],
         [-0.8438,  0.0537, -1.2327,  ...,  0.1199, -2.1162,  1.1981],
         [ 0.5108,  1.5178, -2.0813,  ...,  1.9069, -1.4145, -0.5668],
         ...,
         [ 0.8101,  0.0238, -0.6053,  ...,  2.0544,  0.8680,  2.1614],
         [-0.4096,  1.0482, -1.4425,  ...,  1.1672, -0.5692,  1.5194],
         [-0.0253,  0.0369, -0.0603,  ...,  1.4863,  0.6179,  1.3138]],

        [[ 0.0481, -0.0059,  1.3714,  ...,  2.0192, -0.1682,  2.2117],
         [-0.0953, -0.4966, -0.5360,  ...,  0.3987, -1.3857,  0.4537],
         [ 0.3287,  0.5718,  0.4394,  ...,  1.4992,  1.0485,  1.5313],
         ...,
         [ 1.2738, -0.1531,  0.0572,  ..., -0.4326, -0.6542,  1.1317],
         [-0.6633,  1.3100, -0.7335,  ...,  2.5477, -1.4627,  1.2341],
         [-0.1837,  1.2708, -1.4987,  ...,  0.3473,  0.7090, -0.3562]],

        [[ 0.5224, -0.7277,  1.0824,  ...,  0.5845, -0.1031,  1.9104],
         [ 0.2294, -0.5360, -1.1531,  ...,  1.8841, -0.2558,  0.7341],
         [ 1.4047, -0.9730, -1.0552,  ...,  1.3335,  0.9415,  0.9369],
         ...,
         [-0.6614, -0.9995,  0.8481,  ...,  1.6161, -0.2282,  2.1447],
         [-0.0342, -1.1429,  0.5810,  ...,  2.8291, -0.5274,  0.1713],
         [ 0.2126, -0.1122,  0.7072,  ...,  1.7862, -0.1233, -0.2301]],

        ...,

        [[-0.8488,  0.0278, -1.3867,  ...,  1.6543,  0.1000,  1.0783],
         [-0.7906, -0.3282, -1.7912,  ...,  0.9759,  0.0615,  1.0441],
         [-0.8393, -0.2167, -1.8490,  ...,  1.4027, -0.0242,  1.3311],
         ...,
         [-0.5508, -0.0167, -1.1446,  ...,  1.3620,  0.1369, -0.0946],
         [-0.7970, -0.6210, -1.8760,  ...,  1.7559, -0.1019,  1.1376],
         [-0.5255, -0.1512, -2.0755,  ...,  1.3972,  0.4904,  1.2905]],

        [[-1.1516, -1.2201, -0.8882,  ...,  1.9287, -0.0687,  1.0847],
         [-1.1392, -1.1468, -1.4747,  ...,  1.0676, -0.5306,  1.3754],
         [-0.8695, -0.9270, -1.1077,  ...,  1.8261,  0.2654,  0.9821],
         ...,
         [-1.0596, -1.1919, -0.5265,  ...,  1.5428,  0.1872,  1.2142],
         [-0.9778, -1.1321, -1.1192,  ...,  1.7618, -0.3586,  0.5166],
         [-0.9906, -0.2917, -0.9196,  ...,  1.6775,  0.3121,  1.2158]],

        [[-1.9499, -0.7749,  0.0190,  ...,  1.5704, -0.1017,  1.0198],
         [-0.6509, -0.0427, -0.2077,  ...,  0.2975, -0.2690,  1.2220],
         [-1.6719, -1.2111, -0.2736,  ...,  0.6374, -0.7388,  0.8894],
         ...,
         [-0.7452, -0.9722,  0.1082,  ...,  1.6850,  0.2262,  1.0479],
         [-1.7484, -1.0612,  0.1421,  ...,  1.7509,  0.2799,  0.9241],
         [-2.0773,  0.1482, -0.2907,  ...,  1.0302, -0.3271,  0.7171]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
INFO:root:attn scores torch.Size([256, 15, 4])
INFO:root:values torch.Size([256, 15, 128])
INFO:root:scores before mask tensor([[[-7.9521e-01, -3.7368e-01, -9.1048e-01,  8.2144e-01],
         [ 3.5036e-01,  2.5886e-01, -4.1854e-01,  5.5889e-02],
         [-3.4942e-01, -6.5716e-01, -4.1876e-01,  7.8333e-01],
         ...,
         [-4.1582e-01, -2.9973e-01,  3.9596e-01, -4.6577e-01],
         [-5.7891e-01, -2.0320e-01,  1.7596e-01,  5.5933e-01],
         [-6.4870e-01, -1.4979e-01, -3.1141e-03,  8.4385e-01]],

        [[-1.0243e+00,  1.4328e-01, -6.7877e-01, -1.0498e-01],
         [-4.2766e-01, -2.3340e-01,  5.3157e-02, -6.8139e-02],
         [-5.8609e-01, -6.3864e-02,  1.3122e-01,  2.5494e-01],
         ...,
         [-7.4613e-01, -1.0427e-01,  5.1973e-01,  7.0431e-01],
         [-4.5947e-01, -1.3578e-01, -6.6763e-01,  1.1091e-01],
         [-5.8002e-01, -4.5194e-01,  3.6155e-01,  1.9053e-01]],

        [[-3.1769e-01,  3.2098e-01, -7.0818e-01,  3.9435e-01],
         [-5.7235e-01, -3.4436e-01, -1.3144e-02, -2.5694e-01],
         [-1.0850e+00, -3.9421e-01,  8.6642e-01,  7.9297e-01],
         ...,
         [-9.8856e-01, -2.5723e-01,  2.5034e-02,  3.1073e-01],
         [-1.0173e+00, -1.1517e+00, -5.4865e-01,  6.1724e-01],
         [ 3.7471e-01, -5.6222e-02,  1.3418e-01, -5.6732e-02]],

        ...,

        [[-5.1014e-01, -9.0403e-01, -5.8297e-01, -1.8513e-01],
         [ 1.1227e-03, -9.7839e-01, -1.8658e-01, -1.2896e-01],
         [ 1.6056e-02, -8.7894e-01, -5.7802e-01,  1.6354e-02],
         ...,
         [-1.1804e-01, -8.4817e-01, -4.1601e-01,  2.8432e-01],
         [ 3.9377e-02, -8.3153e-01, -3.4690e-01, -1.0887e-01],
         [ 4.1739e-02, -8.8903e-01,  1.7331e-01,  2.6829e-01]],

        [[-5.1847e-01, -9.0224e-01, -2.1504e-01,  2.2942e-01],
         [-1.9549e-01, -7.6425e-01, -3.1431e-01, -2.5069e-01],
         [-1.4495e-01, -9.6031e-01,  1.0691e-03,  5.6170e-03],
         ...,
         [ 1.0906e-01, -9.0876e-01,  1.9590e-01, -1.5399e-01],
         [ 2.5153e-02, -8.9806e-01, -4.8125e-01,  3.2964e-02],
         [-1.3087e-01, -9.9076e-01, -3.0524e-01,  3.5134e-01]],

        [[-4.3381e-01, -7.5607e-01, -4.7512e-01,  3.9082e-01],
         [-7.4737e-02, -5.6625e-01, -2.5295e-01, -1.5814e-01],
         [ 2.6743e-01, -7.7883e-01, -2.0001e-01, -4.0795e-02],
         ...,
         [-3.3727e-01, -1.0775e+00, -2.9900e-01,  3.7445e-02],
         [-7.2527e-02, -9.4678e-01, -1.9295e-01, -5.2704e-02],
         [ 1.6446e-01, -8.8022e-01, -4.2032e-02,  3.2541e-01]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:mask for attn scores torch.Size([15, 256])
INFO:root:mask for attn scores reshape torch.Size([256, 15, 1])
INFO:root:scores after masking torch.Size([256, 15, 4])
INFO:root:scores after masking tensor([[[-0.7952, -0.3737, -0.9105,  0.8214],
         [ 0.3504,  0.2589, -0.4185,  0.0559],
         [-0.3494, -0.6572, -0.4188,  0.7833],
         ...,
         [-0.4158, -0.2997,  0.3960, -0.4658],
         [-0.5789, -0.2032,  0.1760,  0.5593],
         [-0.6487, -0.1498, -0.0031,  0.8438]],

        [[-1.0243,  0.1433, -0.6788, -0.1050],
         [-0.4277, -0.2334,  0.0532, -0.0681],
         [-0.5861, -0.0639,  0.1312,  0.2549],
         ...,
         [-0.7461, -0.1043,  0.5197,  0.7043],
         [-0.4595, -0.1358, -0.6676,  0.1109],
         [-0.5800, -0.4519,  0.3615,  0.1905]],

        [[-0.3177,  0.3210, -0.7082,  0.3944],
         [-0.5724, -0.3444, -0.0131, -0.2569],
         [-1.0850, -0.3942,  0.8664,  0.7930],
         ...,
         [-0.9886, -0.2572,  0.0250,  0.3107],
         [-1.0173, -1.1517, -0.5486,  0.6172],
         [ 0.3747, -0.0562,  0.1342, -0.0567]],

        ...,

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]],

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]],

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]]], device='cuda:0',
       grad_fn=<MaskedFillBackward0>)
INFO:root:scores after softmax torch.Size([256, 15, 4])
INFO:root:scores after softmax tensor([[[0.0301, 0.0349, 0.0242, 0.0587],
         [0.0254, 0.0254, 0.0113, 0.0115],
         [0.0181, 0.0140, 0.0160, 0.0287],
         ...,
         [0.0090, 0.0095, 0.0146, 0.0055],
         [0.0256, 0.0314, 0.0527, 0.0339],
         [0.0066, 0.0112, 0.0091, 0.0193]],

        [[0.0240, 0.0585, 0.0305, 0.0232],
         [0.0117, 0.0155, 0.0180, 0.0101],
         [0.0143, 0.0254, 0.0278, 0.0169],
         ...,
         [0.0065, 0.0116, 0.0166, 0.0176],
         [0.0288, 0.0336, 0.0227, 0.0217],
         [0.0071, 0.0083, 0.0131, 0.0100]],

        [[0.0486, 0.0699, 0.0296, 0.0383],
         [0.0101, 0.0139, 0.0169, 0.0084],
         [0.0087, 0.0183, 0.0580, 0.0290],
         ...,
         [0.0051, 0.0099, 0.0101, 0.0119],
         [0.0165, 0.0122, 0.0255, 0.0359],
         [0.0184, 0.0123, 0.0104, 0.0078]],

        ...,

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
INFO:root:scores after reshape torch.Size([256, 60, 1])
INFO:root:scores after reshape tensor([[[0.0301],
         [0.0349],
         [0.0242],
         ...,
         [0.0112],
         [0.0091],
         [0.0193]],

        [[0.0240],
         [0.0585],
         [0.0305],
         ...,
         [0.0083],
         [0.0131],
         [0.0100]],

        [[0.0486],
         [0.0699],
         [0.0296],
         ...,
         [0.0123],
         [0.0104],
         [0.0078]],

        ...,

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]],

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]],

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)
INFO:root:scores torch.Size([256, 60, 1])
INFO:root:values after shaping torch.Size([256, 60, 32])
INFO:root:tensor([[[-1.0098,  0.5106,  0.2543,  ..., -0.4365,  0.9161,  0.6245],
         [ 1.3222, -0.8254, -0.9557,  ...,  0.1265,  0.4961,  0.1685],
         [-0.4377,  0.3890,  0.0256,  ..., -0.0554,  0.9117,  0.4267],
         ...,
         [ 1.6758, -0.7918,  0.2414,  ..., -0.1052,  1.2539,  0.6132],
         [ 0.4461, -0.7207, -0.3502,  ..., -0.1139,  0.0606, -0.1205],
         [ 0.9841,  1.1259,  0.7205,  ...,  0.6839,  0.0233,  0.1392]],

        [[-0.1939, -0.2354,  0.4770,  ..., -0.1547,  0.3600, -0.4102],
         [ 0.4113, -0.1957,  0.3837,  ..., -0.1123,  0.7374,  0.9653],
         [ 0.2052,  0.0701,  1.4636,  ..., -0.3902, -0.0476,  0.0315],
         ...,
         [ 0.3133, -0.5035, -0.5847,  ...,  0.0795,  0.8360,  0.0950],
         [ 0.5104,  0.4446, -0.2980,  ...,  0.5542, -0.0577,  0.0947],
         [ 1.1574,  0.6921,  0.0092,  ...,  1.0191,  0.1277,  0.0580]],

        [[ 0.0994, -0.0470,  0.3863,  ..., -1.0898, -0.3354, -0.2116],
         [-0.1830, -0.7605, -0.0201,  ...,  0.0353,  0.7182,  0.2309],
         [-0.6438,  0.0020, -0.2183,  ..., -1.4048,  0.0346, -0.5580],
         ...,
         [ 0.1564,  0.0386, -0.3058,  ..., -0.5412,  1.1983,  0.5153],
         [-0.1409,  0.0261,  0.4467,  ..., -1.2466, -0.3301,  0.6977],
         [ 0.8055, -0.1374, -0.9847,  ...,  0.2654, -0.0084, -0.3056]],

        ...,

        [[-0.3826,  0.3665, -0.3952,  ...,  1.4845,  0.3957, -0.1524],
         [ 0.1316,  0.5247,  0.7059,  ..., -0.5070,  0.1351, -0.2658],
         [ 0.0766,  0.1193,  0.3693,  ...,  0.2741, -0.2782, -0.3888],
         ...,
         [-0.6453,  0.2002,  0.5569,  ..., -0.2766,  0.0614,  0.3560],
         [-0.2240,  0.3124,  0.4041,  ...,  0.0402,  0.2237, -0.6406],
         [ 0.7225,  0.6911, -0.2324,  ...,  0.0853, -0.5553,  0.7599]],

        [[ 0.0456,  0.0958, -0.3441,  ...,  1.4492,  0.4023, -0.0556],
         [-0.1446,  0.4349,  0.7232,  ..., -0.3808,  0.4829,  0.3071],
         [ 0.0029, -0.3606,  0.2562,  ...,  0.1294,  0.1270, -0.4793],
         ...,
         [-0.0581,  0.4842,  0.6212,  ..., -0.4671,  0.6202,  0.3863],
         [ 0.0445, -0.3343,  0.2510,  ...,  0.1357,  0.0627, -0.4228],
         [ 0.9652,  0.7900, -0.1514,  ..., -0.0163, -0.2742,  0.8459]],

        [[-0.4181,  0.2748, -0.3858,  ...,  1.1389,  0.1626,  0.0407],
         [ 0.2508,  0.2590,  0.9254,  ..., -0.1055,  0.2728,  0.2477],
         [ 0.0788,  0.0717, -0.1734,  ...,  0.1325,  0.1594, -0.2404],
         ...,
         [ 0.0272,  0.5659,  0.6300,  ..., -0.9490,  0.3368,  0.2948],
         [ 0.1650,  0.1642,  0.2388,  ...,  0.3761, -0.4029,  0.0602],
         [ 0.7246,  0.5263,  0.6853,  ...,  0.1881, -0.1511,  0.8297]]],
       device='cuda:0', grad_fn=<ViewBackward>)
INFO:root:tokens after weightin torch.Size([256, 60, 32])
INFO:root:tensor([[[-3.3807e-02,  1.7092e-02,  8.5124e-03,  ..., -1.4612e-02,
           3.0668e-02,  2.0908e-02],
         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [-1.1778e-02,  1.0465e-02,  6.8885e-04,  ..., -1.4903e-03,
           2.4532e-02,  1.1480e-02],
         ...,
         [ 2.0936e-02, -9.8927e-03,  3.0165e-03,  ..., -1.3139e-03,
           1.5666e-02,  7.6604e-03],
         [ 4.5079e-03, -7.2836e-03, -3.5389e-03,  ..., -1.1508e-03,
           6.1229e-04, -1.2179e-03],
         [ 2.1116e-02,  2.4159e-02,  1.5459e-02,  ...,  1.4674e-02,
           5.0098e-04,  2.9866e-03]],

        [[-5.1621e-03, -6.2672e-03,  1.2699e-02,  ..., -4.1192e-03,
           9.5854e-03, -1.0921e-02],
         [ 2.6740e-02, -1.2723e-02,  2.4943e-02,  ..., -7.2986e-03,
           4.7937e-02,  6.2750e-02],
         [ 6.9618e-03,  2.3767e-03,  4.9648e-02,  ..., -1.3238e-02,
          -1.6164e-03,  1.0696e-03],
         ...,
         [ 2.8937e-03, -4.6501e-03, -5.3999e-03,  ...,  7.3454e-04,
           7.7211e-03,  8.7743e-04],
         [ 7.4272e-03,  6.4703e-03, -4.3371e-03,  ...,  8.0658e-03,
          -8.3950e-04,  1.3784e-03],
         [ 1.2922e-02,  7.7273e-03,  1.0270e-04,  ...,  1.1378e-02,
           1.4254e-03,  6.4721e-04]],

        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [-2.1207e-02,  6.4408e-05, -7.1922e-03,  ..., -4.6275e-02,
           1.1401e-03, -1.8381e-02],
         ...,
         [ 2.1455e-03,  5.2947e-04, -4.1947e-03,  ..., -7.4253e-03,
           1.6440e-02,  7.0689e-03],
         [-1.6331e-03,  3.0252e-04,  5.1793e-03,  ..., -1.4452e-02,
          -3.8275e-03,  8.0893e-03],
         [ 7.0235e-03, -1.1976e-03, -8.5857e-03,  ...,  2.3142e-03,
          -7.3048e-05, -2.6644e-03]],

        ...,

        [[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00]],

        [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00,  0.0000e+00]],

        [[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         ...,
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00]]], device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc representation after sumtorch.Size([60, 32])
INFO:root:tensor([[-0.8067,  0.3528, -0.1772,  ...,  0.3574, -0.5436, -0.1019],
        [ 0.3420, -0.4593,  0.1401,  ...,  0.0299,  0.4229,  0.1764],
        [-0.1226,  0.2120, -0.2693,  ..., -0.4814, -0.0461, -0.1132],
        ...,
        [ 0.3265, -0.6009,  0.1616,  ...,  0.3428,  0.4883, -0.1801],
        [ 0.0327, -0.0871, -0.1573,  ..., -0.5286, -0.1824, -0.2683],
        [ 0.4420,  0.4764, -0.3584,  ...,  0.6832, -0.1880,  0.2472]],
       device='cuda:0', grad_fn=<SumBackward1>)
INFO:root:doc representation reshpae torch.Size([5, 3, 128])
INFO:root:doc representation torch.Size([5, 3, 128])
INFO:root:tensor([[[-0.0032, -0.2837,  0.4370,  ..., -0.0705,  0.3249,  0.2156],
         [-0.0198,  0.1110,  0.4770,  ...,  0.0522,  0.1018,  0.1507],
         [-0.0785, -0.2897,  0.5930,  ..., -0.1024,  0.1809,  0.2506]],

        [[-0.2117, -0.1589,  0.4839,  ..., -0.0444,  0.2792,  0.1384],
         [-0.2438, -0.2448,  0.4885,  ..., -0.0587,  0.2599,  0.1194],
         [-0.2139, -0.2087,  0.4627,  ..., -0.0261,  0.2492,  0.1796]],

        [[-0.1947, -0.1935,  0.5194,  ...,  0.0047,  0.2313,  0.1814],
         [-0.1583, -0.0162,  0.4268,  ...,  0.0252,  0.1607,  0.0875],
         [-0.1538, -0.0767,  0.4034,  ...,  0.0284,  0.1452,  0.0621]],

        [[ 0.1420, -0.2368,  0.4181,  ...,  0.0149,  0.2530,  0.1115],
         [-0.0800,  0.0879,  0.4531,  ...,  0.0247,  0.0774,  0.1626],
         [-0.1151, -0.1473,  0.4477,  ...,  0.0206,  0.3217,  0.2218]],

        [[-0.0205, -0.2313,  0.4558,  ..., -0.2057,  0.2489,  0.1519],
         [-0.0605,  0.1035,  0.4368,  ...,  0.0584,  0.0744,  0.1713],
         [-0.1965, -0.1323,  0.4948,  ..., -0.0485,  0.1075,  0.1470]]],
       device='cuda:0', grad_fn=<AddBackward0>)
WARNING:root:Add normalization after pooling
INFO:root:Pooling size torch.Size([5, 3, 128])
INFO:root:x pooled tensor([[[-0.0032, -0.2837,  0.4370,  ..., -0.0705,  0.3249,  0.2156],
         [-0.0198,  0.1110,  0.4770,  ...,  0.0522,  0.1018,  0.1507],
         [-0.0785, -0.2897,  0.5930,  ..., -0.1024,  0.1809,  0.2506]],

        [[-0.2117, -0.1589,  0.4839,  ..., -0.0444,  0.2792,  0.1384],
         [-0.2438, -0.2448,  0.4885,  ..., -0.0587,  0.2599,  0.1194],
         [-0.2139, -0.2087,  0.4627,  ..., -0.0261,  0.2492,  0.1796]],

        [[-0.1947, -0.1935,  0.5194,  ...,  0.0047,  0.2313,  0.1814],
         [-0.1583, -0.0162,  0.4268,  ...,  0.0252,  0.1607,  0.0875],
         [-0.1538, -0.0767,  0.4034,  ...,  0.0284,  0.1452,  0.0621]],

        [[ 0.1420, -0.2368,  0.4181,  ...,  0.0149,  0.2530,  0.1115],
         [-0.0800,  0.0879,  0.4531,  ...,  0.0247,  0.0774,  0.1626],
         [-0.1151, -0.1473,  0.4477,  ...,  0.0206,  0.3217,  0.2218]],

        [[-0.0205, -0.2313,  0.4558,  ..., -0.2057,  0.2489,  0.1519],
         [-0.0605,  0.1035,  0.4368,  ...,  0.0584,  0.0744,  0.1713],
         [-0.1965, -0.1323,  0.4948,  ..., -0.0485,  0.1075,  0.1470]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:doc attn size torch.Size([5, 3, 128])
INFO:root:inter doc attn tensor([[[ 8.1604e-02,  2.0995e-01, -3.3144e-02,  ...,  5.9132e-02,
          -8.6731e-02, -4.5122e-02],
         [ 4.3249e-02,  1.7486e-01, -9.1085e-02,  ...,  6.6055e-02,
          -9.3960e-02, -1.3342e-01],
         [ 7.4219e-02,  1.7672e-01, -2.8533e-02,  ...,  3.9104e-02,
          -8.2725e-02, -7.6564e-02]],

        [[ 5.9856e-02,  1.8703e-01, -2.5055e-02,  ...,  5.3848e-02,
          -8.1251e-02, -2.3998e-02],
         [ 3.9558e-02,  1.6582e-01, -1.0398e-01,  ...,  6.7154e-02,
          -8.0862e-02, -1.1985e-01],
         [ 8.0595e-02,  2.0196e-01, -4.2979e-02,  ...,  4.6088e-02,
          -9.5428e-02, -8.2609e-02]],

        [[ 8.1526e-02,  2.0982e-01, -3.3086e-02,  ...,  5.9229e-02,
          -8.6742e-02, -4.4993e-02],
         [-9.2268e-05,  1.2845e-01, -6.7903e-02,  ...,  6.7048e-02,
          -6.1973e-02, -8.3547e-02],
         [ 5.3256e-02,  1.7898e-01, -3.6994e-02,  ...,  5.1200e-02,
          -8.6268e-02, -7.1735e-02]],

        [[ 6.4040e-02,  2.0560e-01, -3.4341e-02,  ...,  6.6345e-02,
          -8.8435e-02, -3.2707e-02],
         [ 2.9348e-02,  1.8172e-01, -1.0229e-01,  ...,  7.2291e-02,
          -9.1877e-02, -1.3251e-01],
         [ 9.9835e-02,  1.8654e-01, -3.1929e-02,  ...,  4.6797e-02,
          -8.5636e-02, -7.7337e-02]],

        [[ 6.7499e-02,  1.7705e-01, -2.4469e-02,  ...,  5.0878e-02,
          -8.2090e-02, -5.1093e-02],
         [ 9.1693e-03,  1.4146e-01, -8.4174e-02,  ...,  6.8458e-02,
          -6.9825e-02, -1.1683e-01],
         [ 4.4562e-02,  1.7663e-01, -3.4847e-02,  ...,  5.5416e-02,
          -8.8370e-02, -5.0407e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:mmr!
INFO:root:query size before linear: torch.Size([256, 3, 128])
INFO:root:doc emb torch.Size([5, 3, 128])
WARNING:root:As of 7-8, current issue is when to mask query and when to mask document too
INFO:root:query after linear torch.Size([3, 256, 128]) 
INFO:root:query_mask torch.Size([3, 256])
INFO:root:query_mask tensor([[False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True],
        [False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True],
        [False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True]], device='cuda:0')
INFO:root:query attn after linear: torch.Size([3, 256, 1])
INFO:root:doc emb torch.Size([3, 5, 128])
INFO:root:query doc linear torch.Size([3, 256, 5])
INFO:root:weighted query doc linear torch.Size([3, 256, 5])
INFO:root:tensor([[[-1.2001e-02, -1.2967e-02, -1.2001e-02, -1.1725e-02, -1.3693e-02],
         [-1.2518e-02, -1.1623e-02, -1.2516e-02, -1.1318e-02, -1.4976e-02],
         [-4.4456e-03, -4.9715e-03, -4.4532e-03, -3.6961e-03, -8.5250e-03],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00]],

        [[-5.0979e-04, -3.5190e-03, -2.5431e-03,  1.8696e-05, -4.9210e-03],
         [-9.5522e-03, -1.1600e-02, -1.1802e-02, -9.9357e-03, -1.3928e-02],
         [-5.7612e-03, -5.9182e-03, -4.4664e-03, -5.1173e-03, -5.2296e-03],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]],

        [[-1.3016e-03,  1.6809e-03,  6.7904e-04,  6.9558e-04,  7.5649e-04],
         [-6.9776e-03, -5.1118e-03, -7.0488e-03, -4.4937e-03, -7.7274e-03],
         [-1.9169e-02, -1.7974e-02, -1.6024e-02, -2.0520e-02, -1.5784e-02],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
         [-0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:first sim: torch.Size([3, 5])
INFO:root:tensor([[ 0.1114,  0.0774,  0.1114,  0.1037,  0.0215],
        [ 0.0931,  0.0366,  0.0142,  0.0856, -0.0424],
        [-0.0590, -0.0176, -0.0756,  0.0171, -0.0875]], device='cuda:0',
       grad_fn=<SumBackward1>)
INFO:root:doc weights torch.Size([3, 5, 128])
INFO:root:doc weights tensor([[[ 0.1058,  0.0560, -0.0498,  ..., -0.0584,  0.0471, -0.1311],
         [ 0.0991,  0.0573, -0.0433,  ..., -0.0512,  0.0418, -0.1182],
         [ 0.1058,  0.0559, -0.0498,  ..., -0.0584,  0.0471, -0.1311],
         [ 0.1017,  0.0527, -0.0427,  ..., -0.0592,  0.0455, -0.1171],
         [ 0.0851,  0.0725, -0.0699,  ..., -0.0556,  0.0454, -0.1349]],

        [[ 0.0505,  0.1085, -0.0092,  ..., -0.0469,  0.0483, -0.1334],
         [ 0.0404,  0.1077, -0.0217,  ..., -0.0509,  0.0480, -0.1328],
         [ 0.0576,  0.1056, -0.0179,  ..., -0.0399,  0.0452, -0.1120],
         [ 0.0524,  0.1094, -0.0181,  ..., -0.0485,  0.0508, -0.1336],
         [ 0.0487,  0.1135, -0.0367,  ..., -0.0366,  0.0350, -0.1330]],

        [[ 0.0787,  0.0954, -0.0390,  ..., -0.0611,  0.0437, -0.1318],
         [ 0.0768,  0.0903, -0.0388,  ..., -0.0696,  0.0492, -0.1362],
         [ 0.0761,  0.0953, -0.0428,  ..., -0.0654,  0.0421, -0.1195],
         [ 0.0917,  0.0925, -0.0244,  ..., -0.0546,  0.0445, -0.1285],
         [ 0.0725,  0.0893, -0.0395,  ..., -0.0686,  0.0449, -0.1061]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:sim 2 torch.Size([3, 5, 5])
INFO:root:sim 2 tensor([[[0.8352, 0.7739, 0.8351, 0.8098, 0.7768],
         [0.7739, 0.7234, 0.7738, 0.7532, 0.7206],
         [0.8351, 0.7738, 0.8350, 0.8097, 0.7767],
         [0.8098, 0.7532, 0.8097, 0.7902, 0.7469],
         [0.7768, 0.7206, 0.7767, 0.7469, 0.7454]],

        [[0.8071, 0.7903, 0.7042, 0.8215, 0.7568],
         [0.7903, 0.7832, 0.6886, 0.8062, 0.7457],
         [0.7042, 0.6886, 0.6329, 0.7179, 0.6707],
         [0.8215, 0.8062, 0.7179, 0.8379, 0.7729],
         [0.7568, 0.7457, 0.6707, 0.7729, 0.7272]],

        [[0.7820, 0.8198, 0.7640, 0.8180, 0.7350],
         [0.8198, 0.8668, 0.8058, 0.8601, 0.7765],
         [0.7640, 0.8058, 0.7554, 0.7977, 0.7305],
         [0.8180, 0.8601, 0.7977, 0.8664, 0.7669],
         [0.7350, 0.7765, 0.7305, 0.7669, 0.7109]]], device='cuda:0',
       grad_fn=<BmmBackward>)
INFO:root:mmr_scores torch.Size([3, 5, 5])
INFO:root:mmr_scores tensor([[[-0.3619, -0.3313, -0.3619, -0.3492, -0.3327],
         [-0.3483, -0.3230, -0.3482, -0.3379, -0.3216],
         [-0.3619, -0.3312, -0.3618, -0.3492, -0.3327],
         [-0.3530, -0.3247, -0.3530, -0.3432, -0.3216],
         [-0.3777, -0.3496, -0.3776, -0.3627, -0.3620]],

        [[-0.3570, -0.3486, -0.3056, -0.3642, -0.3319],
         [-0.3768, -0.3733, -0.3260, -0.3848, -0.3546],
         [-0.3450, -0.3372, -0.3093, -0.3518, -0.3282],
         [-0.3679, -0.3603, -0.3161, -0.3762, -0.3436],
         [-0.3996, -0.3941, -0.3565, -0.4076, -0.3848]],

        [[-0.4205, -0.4394, -0.4115, -0.4385, -0.3970],
         [-0.4187, -0.4422, -0.4117, -0.4388, -0.3970],
         [-0.4198, -0.4407, -0.4155, -0.4367, -0.4030],
         [-0.4005, -0.4215, -0.3903, -0.4247, -0.3749],
         [-0.4112, -0.4320, -0.4090, -0.4272, -0.3992]]], device='cuda:0',
       grad_fn=<SubBackward0>)
INFO:root:mmr_scores torch.Size([3, 5])
INFO:root:tensor([[[ 8.1604e-02,  2.0995e-01, -3.3144e-02,  ...,  5.9132e-02,
          -8.6731e-02, -4.5122e-02],
         [ 5.9856e-02,  1.8703e-01, -2.5055e-02,  ...,  5.3848e-02,
          -8.1251e-02, -2.3998e-02],
         [ 8.1526e-02,  2.0982e-01, -3.3086e-02,  ...,  5.9229e-02,
          -8.6742e-02, -4.4993e-02],
         [ 6.4040e-02,  2.0560e-01, -3.4341e-02,  ...,  6.6345e-02,
          -8.8435e-02, -3.2707e-02],
         [ 6.7499e-02,  1.7705e-01, -2.4469e-02,  ...,  5.0878e-02,
          -8.2090e-02, -5.1093e-02]],

        [[ 4.3249e-02,  1.7486e-01, -9.1085e-02,  ...,  6.6055e-02,
          -9.3960e-02, -1.3342e-01],
         [ 3.9558e-02,  1.6582e-01, -1.0398e-01,  ...,  6.7154e-02,
          -8.0862e-02, -1.1985e-01],
         [-9.2268e-05,  1.2845e-01, -6.7903e-02,  ...,  6.7048e-02,
          -6.1973e-02, -8.3547e-02],
         [ 2.9348e-02,  1.8172e-01, -1.0229e-01,  ...,  7.2291e-02,
          -9.1877e-02, -1.3251e-01],
         [ 9.1693e-03,  1.4146e-01, -8.4174e-02,  ...,  6.8458e-02,
          -6.9825e-02, -1.1683e-01]],

        [[ 7.4219e-02,  1.7672e-01, -2.8533e-02,  ...,  3.9104e-02,
          -8.2725e-02, -7.6564e-02],
         [ 8.0595e-02,  2.0196e-01, -4.2979e-02,  ...,  4.6088e-02,
          -9.5428e-02, -8.2609e-02],
         [ 5.3256e-02,  1.7898e-01, -3.6994e-02,  ...,  5.1200e-02,
          -8.6268e-02, -7.1735e-02],
         [ 9.9835e-02,  1.8654e-01, -3.1929e-02,  ...,  4.6797e-02,
          -8.5636e-02, -7.7337e-02],
         [ 4.4562e-02,  1.7663e-01, -3.4847e-02,  ...,  5.5416e-02,
          -8.8370e-02, -5.0407e-02]]], device='cuda:0',
       grad_fn=<TransposeBackward0>)
INFO:root:tensor([[[ 1.6585e-02,  4.2669e-02, -6.7361e-03,  ...,  1.2018e-02,
          -1.7627e-02, -9.1704e-03],
         [ 1.2141e-02,  3.7938e-02, -5.0822e-03,  ...,  1.0923e-02,
          -1.6481e-02, -4.8678e-03],
         [ 1.6569e-02,  4.2642e-02, -6.7243e-03,  ...,  1.2037e-02,
          -1.7629e-02, -9.1441e-03],
         [ 1.3033e-02,  4.1843e-02, -6.9890e-03,  ...,  1.3502e-02,
          -1.7998e-02, -6.6565e-03],
         [ 1.3721e-02,  3.5990e-02, -4.9740e-03,  ...,  1.0342e-02,
          -1.6687e-02, -1.0386e-02]],

        [[ 8.9636e-03,  3.6241e-02, -1.8878e-02,  ...,  1.3690e-02,
          -1.9474e-02, -2.7653e-02],
         [ 8.2089e-03,  3.4410e-02, -2.1578e-02,  ...,  1.3936e-02,
          -1.6780e-02, -2.4870e-02],
         [-1.8918e-05,  2.6338e-02, -1.3923e-02,  ...,  1.3747e-02,
          -1.2707e-02, -1.7130e-02],
         [ 6.0877e-03,  3.7693e-02, -2.1219e-02,  ...,  1.4995e-02,
          -1.9058e-02, -2.7487e-02],
         [ 1.8932e-03,  2.9206e-02, -1.7379e-02,  ...,  1.4134e-02,
          -1.4417e-02, -2.4123e-02]],

        [[ 1.5208e-02,  3.6211e-02, -5.8466e-03,  ...,  8.0128e-03,
          -1.6951e-02, -1.5689e-02],
         [ 1.6519e-02,  4.1394e-02, -8.8091e-03,  ...,  9.4465e-03,
          -1.9559e-02, -1.6932e-02],
         [ 1.0866e-02,  3.6520e-02, -7.5483e-03,  ...,  1.0447e-02,
          -1.7602e-02, -1.4637e-02],
         [ 2.0519e-02,  3.8339e-02, -6.5624e-03,  ...,  9.6183e-03,
          -1.7601e-02, -1.5895e-02],
         [ 9.0602e-03,  3.5912e-02, -7.0850e-03,  ...,  1.1267e-02,
          -1.7967e-02, -1.0249e-02]]], device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc_emb torch.Size([3, 5, 128])
INFO:root:doc mmr weighted size torch.Size([5, 3, 128])
INFO:root:input for adding context torch.Size([256, 5, 3, 128])
DEBUG:root:local doc emb size after global enc torch.Size([256, 15, 128]):
DEBUG:root:global doc emb size after global_enc torch.Size([5, 3, 128]):
DEBUG:root:query emb size after global_enc torch.Size([256, 3, 128]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([256, 15, 128])
INFO:root:query size torch.Size([256, 3, 128])
INFO:root:src padding_mask size torch.Size([15, 256])
INFO:root:Computing multi-head pooling
INFO:root:tensor([[[ 2.1557e-01,  1.4276e-01, -1.3550e+00,  ...,  3.1895e+00,
          -1.3439e+00,  8.0545e-01],
         [-8.8204e-01, -1.8537e-01, -1.0991e+00,  ...,  5.7492e-01,
          -2.0431e+00,  1.5156e+00],
         [ 3.9769e-01,  1.4815e+00, -1.6972e+00,  ...,  2.3326e+00,
          -1.2310e+00, -2.8743e-01],
         ...,
         [ 4.0736e-01,  8.6057e-02, -4.0128e-01,  ...,  2.8010e+00,
           9.1649e-01,  2.0541e+00],
         [-4.8863e-01,  6.0928e-01, -1.4769e+00,  ...,  1.9415e+00,
          -5.3743e-01,  1.5691e+00],
         [-2.2642e-01, -6.9626e-02, -2.8063e-01,  ...,  2.1359e+00,
           1.1548e+00,  1.1717e+00]],

        [[-2.4686e-01, -1.0430e-02,  1.6487e+00,  ...,  2.6155e+00,
          -3.1094e-02,  2.2531e+00],
         [-1.9165e-01, -3.7491e-01, -3.8508e-01,  ...,  3.7437e-01,
          -9.3869e-01, -2.6351e-02],
         [ 1.3332e-01,  3.7704e-01,  1.3739e-02,  ...,  2.0543e+00,
           1.0240e+00,  1.7256e+00],
         ...,
         [ 1.2415e+00, -5.0093e-01, -1.4286e-01,  ...,  1.0828e-01,
          -1.6971e-01,  1.0435e+00],
         [-6.6800e-01,  1.2640e+00, -6.9601e-01,  ...,  2.5076e+00,
          -1.6014e+00,  1.2267e+00],
         [-5.5437e-01,  1.3146e+00, -1.3473e+00,  ...,  8.3615e-01,
           1.2064e+00, -3.2538e-01]],

        [[ 2.3188e-02, -4.8518e-01,  1.2385e+00,  ...,  1.4272e+00,
           5.2820e-02,  1.7496e+00],
         [ 2.3335e-01, -4.3873e-01, -1.1200e+00,  ...,  1.9821e+00,
           2.7669e-01,  8.8396e-01],
         [ 1.3187e+00, -1.2605e+00, -8.5670e-01,  ...,  1.9894e+00,
           1.1323e+00,  6.1159e-01],
         ...,
         [-6.5058e-01, -7.6161e-01,  8.7514e-01,  ...,  2.0943e+00,
          -3.7049e-02,  2.0388e+00],
         [-1.2519e-01, -8.5260e-01,  5.3958e-01,  ...,  3.4905e+00,
          -1.7059e-01,  1.9422e-01],
         [ 2.0319e-01, -1.0830e-01,  9.0909e-01,  ...,  2.1401e+00,
           2.3206e-02, -2.1476e-01]],

        ...,

        [[-6.3071e-01,  1.7497e-02, -1.3095e+00,  ...,  2.0677e+00,
           7.0647e-02,  1.0952e+00],
         [-9.2856e-01, -4.2173e-01, -1.5561e+00,  ...,  1.5206e+00,
          -3.2777e-01,  1.0310e+00],
         [-8.3097e-01, -1.0758e-01, -1.4578e+00,  ...,  1.8820e+00,
           7.2287e-02,  1.2254e+00],
         ...,
         [-5.1415e-01,  6.1491e-03, -1.0040e+00,  ...,  1.8864e+00,
           1.1901e-01, -2.7827e-03],
         [-1.0631e+00, -6.2732e-01, -1.6398e+00,  ...,  2.1851e+00,
          -1.3746e-01,  9.4688e-01],
         [-5.5416e-01, -9.5484e-02, -1.6194e+00,  ...,  2.1113e+00,
           4.7475e-01,  1.2871e+00]],

        [[-1.0953e+00, -1.1638e+00, -8.5737e-01,  ...,  2.5237e+00,
           1.7773e-02,  9.3599e-01],
         [-1.1599e+00, -1.1878e+00, -1.3587e+00,  ...,  1.4119e+00,
          -6.5184e-01,  1.1163e+00],
         [-8.5966e-01, -9.1361e-01, -1.0035e+00,  ...,  2.4586e+00,
           2.8947e-01,  6.5355e-01],
         ...,
         [-1.2918e+00, -1.1606e+00, -4.0661e-01,  ...,  2.1421e+00,
           2.1835e-01,  1.2898e+00],
         [-1.2182e+00, -9.0733e-01, -9.6408e-01,  ...,  1.7141e+00,
          -3.4178e-01,  5.4731e-01],
         [-1.0670e+00, -1.0153e-01, -7.2208e-01,  ...,  2.1701e+00,
          -1.2245e-01,  1.2413e+00]],

        [[-1.8348e+00, -7.0116e-01,  3.0889e-01,  ...,  2.4256e+00,
          -2.1917e-01,  9.9828e-01],
         [-7.5441e-01, -8.1439e-02,  1.4173e-01,  ...,  1.0514e+00,
          -2.9213e-01,  1.0626e+00],
         [-1.6308e+00, -1.2282e+00, -2.5061e-01,  ...,  1.3714e+00,
          -7.5090e-01,  8.1624e-01],
         ...,
         [-5.7274e-01, -9.3124e-01,  2.3842e-01,  ...,  2.3846e+00,
           1.5811e-02,  1.2219e+00],
         [-1.7320e+00, -1.0064e+00,  1.1102e-01,  ...,  2.4216e+00,
           1.4324e-01,  1.1517e+00],
         [-1.8447e+00,  1.7990e-01,  9.9896e-02,  ...,  1.6093e+00,
          -4.1921e-01,  7.7955e-01]]], device='cuda:0', grad_fn=<ViewBackward>)
INFO:root:attn scores torch.Size([256, 15, 4])
INFO:root:values torch.Size([256, 15, 128])
INFO:root:scores before mask tensor([[[-0.7446, -0.4494, -0.8715,  0.9723],
         [ 0.3428,  0.0551, -0.5039,  0.1880],
         [-0.1851, -0.6909, -0.5092,  0.9846],
         ...,
         [-0.3483, -0.5228,  0.3643, -0.2718],
         [-0.4539, -0.3603,  0.1097,  0.5315],
         [-0.6561, -0.2305,  0.1211,  0.9302]],

        [[-1.0005, -0.0770, -0.9987,  0.0436],
         [-0.3356, -0.4504, -0.0364, -0.0633],
         [-0.4772, -0.0818, -0.1289,  0.2224],
         ...,
         [-0.7765, -0.1943,  0.3214,  0.6313],
         [-0.3697, -0.3203, -0.6404,  0.1354],
         [-0.5236, -0.4619,  0.2503,  0.4004]],

        [[-0.2356,  0.1062, -0.8093,  0.6254],
         [-0.1547, -0.4168, -0.0491, -0.2302],
         [-0.9506, -0.3160,  0.6348,  0.8614],
         ...,
         [-0.7788, -0.2947, -0.1986,  0.3218],
         [-0.8840, -1.1271, -0.6884,  0.4882],
         [ 0.4960, -0.2694,  0.0197,  0.0720]],

        ...,

        [[-0.3798, -0.9526, -0.5386, -0.0238],
         [ 0.0581, -1.0516, -0.1573,  0.0487],
         [ 0.0546, -0.9038, -0.6853,  0.2020],
         ...,
         [-0.1180, -0.9376, -0.3852,  0.4775],
         [ 0.0652, -0.8322, -0.4386,  0.2054],
         [ 0.0300, -0.9977,  0.2335,  0.3728]],

        [[-0.4404, -1.0652, -0.1746,  0.2887],
         [-0.1481, -0.7336, -0.4551, -0.1227],
         [-0.0603, -0.9869, -0.1508,  0.1902],
         ...,
         [ 0.1254, -1.0016,  0.3827, -0.0937],
         [ 0.0207, -0.8936, -0.4416,  0.3173],
         [-0.1341, -0.9889, -0.3304,  0.4989]],

        [[-0.4269, -0.7685, -0.4159,  0.5210],
         [-0.0029, -0.6364, -0.2988,  0.0940],
         [ 0.3042, -0.8203, -0.1143,  0.1894],
         ...,
         [-0.1816, -1.0926, -0.3709,  0.3012],
         [-0.0826, -1.0162, -0.1086,  0.0791],
         [ 0.1206, -0.9686, -0.1421,  0.4987]]], device='cuda:0',
       grad_fn=<AddBackward0>)
INFO:root:mask for attn scores torch.Size([15, 256])
INFO:root:mask for attn scores reshape torch.Size([256, 15, 1])
INFO:root:scores after masking torch.Size([256, 15, 4])
INFO:root:scores after masking tensor([[[-0.7446, -0.4494, -0.8715,  0.9723],
         [ 0.3428,  0.0551, -0.5039,  0.1880],
         [-0.1851, -0.6909, -0.5092,  0.9846],
         ...,
         [-0.3483, -0.5228,  0.3643, -0.2718],
         [-0.4539, -0.3603,  0.1097,  0.5315],
         [-0.6561, -0.2305,  0.1211,  0.9302]],

        [[-1.0005, -0.0770, -0.9987,  0.0436],
         [-0.3356, -0.4504, -0.0364, -0.0633],
         [-0.4772, -0.0818, -0.1289,  0.2224],
         ...,
         [-0.7765, -0.1943,  0.3214,  0.6313],
         [-0.3697, -0.3203, -0.6404,  0.1354],
         [-0.5236, -0.4619,  0.2503,  0.4004]],

        [[-0.2356,  0.1062, -0.8093,  0.6254],
         [-0.1547, -0.4168, -0.0491, -0.2302],
         [-0.9506, -0.3160,  0.6348,  0.8614],
         ...,
         [-0.7788, -0.2947, -0.1986,  0.3218],
         [-0.8840, -1.1271, -0.6884,  0.4882],
         [ 0.4960, -0.2694,  0.0197,  0.0720]],

        ...,

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]],

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]],

        [[   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         ...,
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf]]], device='cuda:0',
       grad_fn=<MaskedFillBackward0>)
INFO:root:scores after softmax torch.Size([256, 15, 4])
INFO:root:scores after softmax tensor([[[0.0277, 0.0354, 0.0250, 0.0661],
         [0.0234, 0.0217, 0.0104, 0.0124],
         [0.0198, 0.0144, 0.0148, 0.0322],
         ...,
         [0.0088, 0.0083, 0.0140, 0.0063],
         [0.0275, 0.0294, 0.0489, 0.0314],
         [0.0058, 0.0112, 0.0100, 0.0194]],

        [[0.0214, 0.0514, 0.0220, 0.0261],
         [0.0119, 0.0131, 0.0166, 0.0097],
         [0.0148, 0.0264, 0.0216, 0.0150],
         ...,
         [0.0057, 0.0115, 0.0134, 0.0154],
         [0.0299, 0.0306, 0.0231, 0.0211],
         [0.0066, 0.0089, 0.0114, 0.0114]],

        [[0.0460, 0.0617, 0.0266, 0.0467],
         [0.0142, 0.0136, 0.0164, 0.0082],
         [0.0092, 0.0209, 0.0464, 0.0285],
         ...,
         [0.0057, 0.0104, 0.0080, 0.0113],
         [0.0179, 0.0137, 0.0220, 0.0300],
         [0.0184, 0.0108, 0.0090, 0.0082]],

        ...,

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
INFO:root:scores after reshape torch.Size([256, 60, 1])
INFO:root:scores after reshape tensor([[[0.0277],
         [0.0354],
         [0.0250],
         ...,
         [0.0112],
         [0.0100],
         [0.0194]],

        [[0.0214],
         [0.0514],
         [0.0220],
         ...,
         [0.0089],
         [0.0114],
         [0.0114]],

        [[0.0460],
         [0.0617],
         [0.0266],
         ...,
         [0.0108],
         [0.0090],
         [0.0082]],

        ...,

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]],

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]],

        [[0.0000],
         [0.0000],
         [0.0000],
         ...,
         [0.0000],
         [0.0000],
         [0.0000]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)
INFO:root:scores torch.Size([256, 60, 1])
INFO:root:values after shaping torch.Size([256, 60, 32])
INFO:root:tensor([[[-1.1913,  0.6744,  0.3028,  ..., -0.3633,  0.8753,  0.6126],
         [ 1.6611, -0.8389, -0.7591,  ...,  0.2102,  0.4765,  0.1301],
         [-0.3041,  0.3477, -0.2274,  ..., -0.1342,  1.0923,  0.4507],
         ...,
         [ 1.8493, -0.9484,  0.2838,  ..., -0.1673,  1.2498,  0.5979],
         [ 0.3996, -0.6358, -0.5246,  ..., -0.0924,  0.2071,  0.0321],
         [ 0.7755,  1.1753,  0.5843,  ...,  0.5309, -0.1396,  0.1963]],

        [[-0.3885, -0.0361,  0.5166,  ..., -0.1195,  0.3798, -0.2512],
         [ 0.6981, -0.1194,  0.1844,  ..., -0.1371,  0.8199,  0.8767],
         [ 0.1878,  0.0546,  1.4826,  ..., -0.3607,  0.2450,  0.2930],
         ...,
         [ 0.5100, -0.4570, -0.4847,  ...,  0.1297,  0.7778,  0.2597],
         [ 0.3944,  0.4070, -0.4208,  ...,  0.5622,  0.0471,  0.0457],
         [ 1.2929,  0.7260,  0.0032,  ...,  1.2005, -0.0243,  0.1987]],

        [[-0.1204,  0.1370,  0.4707,  ..., -0.9368, -0.3190, -0.1091],
         [-0.0324, -0.9147,  0.0650,  ...,  0.0220,  0.6522,  0.4237],
         [-0.4998,  0.1637, -0.3635,  ..., -1.4006,  0.1655, -0.2893],
         ...,
         [ 0.2699,  0.0533, -0.4450,  ..., -0.6092,  1.0327,  0.6408],
         [-0.1854,  0.0751,  0.3449,  ..., -1.1745, -0.0615,  0.7933],
         [ 0.7059, -0.0394, -1.1411,  ...,  0.3584, -0.2089, -0.2715]],

        ...,

        [[-0.5577,  0.3909, -0.3500,  ...,  1.4892,  0.2191, -0.0526],
         [ 0.2644,  0.5861,  0.4129,  ..., -0.3847, -0.1526, -0.0948],
         [ 0.1557,  0.0892, -0.0507,  ...,  0.0309,  0.0188, -0.3781],
         ...,
         [-0.6142,  0.3275,  0.3599,  ..., -0.2772, -0.1092,  0.3209],
         [-0.2159,  0.4736,  0.0815,  ..., -0.1248,  0.3627, -0.4869],
         [ 0.7665,  0.7986, -0.3364,  ...,  0.1179, -0.4432,  0.6405]],

        [[-0.0079,  0.1336, -0.3634,  ...,  1.6523,  0.1934, -0.0456],
         [ 0.0261,  0.5662,  0.4548,  ..., -0.4359,  0.2547,  0.3901],
         [ 0.2201, -0.2950, -0.0634,  ...,  0.0055,  0.3972, -0.2948],
         ...,
         [ 0.0803,  0.4286,  0.5071,  ..., -0.5504,  0.4316,  0.4375],
         [ 0.1009, -0.1763,  0.0568,  ..., -0.0543,  0.2183, -0.4220],
         [ 0.9093,  0.8115, -0.2900,  ...,  0.0301, -0.3268,  0.7667]],

        [[-0.6630,  0.4817, -0.3839,  ...,  1.1545,  0.1080,  0.2364],
         [ 0.5351,  0.2228,  0.7146,  ..., -0.3028,  0.0947,  0.3671],
         [ 0.1143,  0.1987, -0.2331,  ...,  0.0147,  0.2998, -0.0851],
         ...,
         [ 0.3503,  0.5267,  0.5346,  ..., -0.8582,  0.1650,  0.2213],
         [ 0.2102,  0.2610, -0.1578,  ...,  0.0189, -0.3201,  0.1334],
         [ 0.7688,  0.7297,  0.4950,  ...,  0.3280, -0.1153,  0.5723]]],
       device='cuda:0', grad_fn=<ViewBackward>)
INFO:root:tokens after weightin torch.Size([256, 60, 32])
INFO:root:tensor([[[-3.6607e-02,  2.0724e-02,  9.3043e-03,  ..., -1.1165e-02,
           2.6897e-02,  1.8824e-02],
         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [-8.4507e-03,  9.6635e-03, -6.3184e-03,  ..., -3.7307e-03,
           3.0355e-02,  1.2526e-02],
         ...,
         [ 2.3064e-02, -1.1828e-02,  3.5391e-03,  ..., -2.0863e-03,
           1.5586e-02,  7.4566e-03],
         [ 4.4396e-03, -7.0626e-03, -5.8276e-03,  ..., -1.0261e-03,
           2.3009e-03,  3.5695e-04],
         [ 1.6732e-02,  2.5359e-02,  1.2607e-02,  ...,  1.1455e-02,
          -3.0127e-03,  4.2351e-03]],

        [[-9.2415e-03, -8.5972e-04,  1.2289e-02,  ..., -2.8429e-03,
           9.0341e-03, -5.9753e-03],
         [ 3.9868e-02, -6.8177e-03,  1.0529e-02,  ..., -7.8315e-03,
           4.6823e-02,  5.0064e-02],
         [ 4.5959e-03,  1.3366e-03,  3.6282e-02,  ..., -8.8266e-03,
           5.9952e-03,  7.1710e-03],
         ...,
         [ 5.0470e-03, -4.5221e-03, -4.7961e-03,  ...,  1.2839e-03,
           7.6971e-03,  2.5702e-03],
         [ 4.9862e-03,  5.1450e-03, -5.3193e-03,  ...,  7.1067e-03,
           5.9554e-04,  5.7808e-04],
         [ 1.6424e-02,  9.2233e-03,  4.0019e-05,  ...,  1.5251e-02,
          -3.0839e-04,  2.5241e-03]],

        [[-6.1530e-03,  7.0015e-03,  2.4059e-02,  ..., -4.7885e-02,
          -1.6306e-02, -5.5752e-03],
         [-2.2214e-03, -6.2738e-02,  4.4600e-03,  ...,  1.5117e-03,
           4.4737e-02,  2.9062e-02],
         [-1.4780e-02,  4.8423e-03, -1.0749e-02,  ..., -4.1421e-02,
           4.8934e-03, -8.5545e-03],
         ...,
         [ 3.2377e-03,  6.3986e-04, -5.3385e-03,  ..., -7.3084e-03,
           1.2389e-02,  7.6870e-03],
         [-1.8609e-03,  7.5407e-04,  3.4618e-03,  ..., -1.1790e-02,
          -6.1744e-04,  7.9628e-03],
         [ 6.4573e-03, -3.6019e-04, -1.0438e-02,  ...,  3.2788e-03,
          -1.9113e-03, -2.4834e-03]],

        ...,

        [[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00,  0.0000e+00],
         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00]],

        [[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         ...,
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00, -0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00]],

        [[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00, -0.0000e+00],
         ...,
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,
           0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00],
         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          -0.0000e+00,  0.0000e+00]]], device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc representation after sumtorch.Size([60, 32])
INFO:root:tensor([[-0.8739,  0.3985, -0.1980,  ...,  0.3462, -0.6628, -0.0968],
        [ 0.3751, -0.5362,  0.1884,  ...,  0.1046,  0.4605,  0.2985],
        [-0.0396,  0.1710, -0.4558,  ..., -0.4361,  0.0888,  0.0128],
        ...,
        [ 0.4065, -0.5678,  0.1882,  ...,  0.3292,  0.3941, -0.0329],
        [ 0.0856, -0.0851, -0.2742,  ..., -0.4798, -0.0671, -0.1972],
        [ 0.3837,  0.4029, -0.4576,  ...,  0.4800, -0.2590,  0.3108]],
       device='cuda:0', grad_fn=<SumBackward1>)
INFO:root:doc representation reshpae torch.Size([5, 3, 128])
INFO:root:doc representation torch.Size([5, 3, 128])
INFO:root:tensor([[[ 1.6895e-02, -3.3487e-01,  5.0040e-01,  ...,  2.5520e-02,
           2.7523e-01,  1.6895e-01],
         [-9.9300e-03,  6.1189e-02,  4.9806e-01,  ...,  1.6998e-01,
           1.3527e-01,  1.4457e-01],
         [-7.0966e-02, -3.0129e-01,  5.9116e-01,  ...,  2.3144e-03,
           2.4958e-01,  2.3915e-01]],

        [[-2.0591e-01, -1.6907e-01,  4.5383e-01,  ...,  4.8147e-02,
           3.0139e-01,  1.2058e-01],
         [-1.8478e-01, -2.4076e-01,  4.8099e-01,  ...,  5.3799e-02,
           2.4712e-01,  1.3174e-01],
         [-1.8702e-01, -2.5444e-01,  4.9213e-01,  ...,  5.9803e-02,
           2.5724e-01,  2.2103e-01]],

        [[-1.9578e-01, -2.0277e-01,  5.1619e-01,  ...,  9.8233e-02,
           2.6353e-01,  1.9687e-01],
         [-1.1487e-01, -7.3682e-02,  4.4345e-01,  ...,  1.2988e-01,
           1.9194e-01,  9.9463e-02],
         [-1.5395e-01, -1.2753e-01,  3.9934e-01,  ...,  1.5524e-01,
           2.1006e-01,  1.0201e-01]],

        [[ 8.1586e-02, -2.7385e-01,  3.8123e-01,  ...,  4.7080e-02,
           2.3251e-01,  1.4431e-01],
         [-3.3494e-02,  5.3905e-02,  5.1104e-01,  ...,  1.4085e-01,
           1.1064e-01,  1.6878e-01],
         [-3.8856e-02, -2.1331e-01,  4.3943e-01,  ...,  1.2741e-01,
           3.7013e-01,  1.8242e-01]],

        [[ 5.0107e-02, -2.3470e-01,  4.5340e-01,  ..., -3.2213e-04,
           2.1187e-01,  1.2378e-01],
         [-3.1714e-02,  6.7890e-02,  4.9021e-01,  ...,  1.8232e-01,
           9.6012e-02,  1.7080e-01],
         [-1.4558e-01, -1.5343e-01,  4.8996e-01,  ...,  6.6474e-02,
           1.3925e-01,  1.5394e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
WARNING:root:Add normalization after pooling
INFO:root:Pooling size torch.Size([5, 3, 128])
INFO:root:x pooled tensor([[[ 1.6895e-02, -3.3487e-01,  5.0040e-01,  ...,  2.5520e-02,
           2.7523e-01,  1.6895e-01],
         [-9.9300e-03,  6.1189e-02,  4.9806e-01,  ...,  1.6998e-01,
           1.3527e-01,  1.4457e-01],
         [-7.0966e-02, -3.0129e-01,  5.9116e-01,  ...,  2.3144e-03,
           2.4958e-01,  2.3915e-01]],

        [[-2.0591e-01, -1.6907e-01,  4.5383e-01,  ...,  4.8147e-02,
           3.0139e-01,  1.2058e-01],
         [-1.8478e-01, -2.4076e-01,  4.8099e-01,  ...,  5.3799e-02,
           2.4712e-01,  1.3174e-01],
         [-1.8702e-01, -2.5444e-01,  4.9213e-01,  ...,  5.9803e-02,
           2.5724e-01,  2.2103e-01]],

        [[-1.9578e-01, -2.0277e-01,  5.1619e-01,  ...,  9.8233e-02,
           2.6353e-01,  1.9687e-01],
         [-1.1487e-01, -7.3682e-02,  4.4345e-01,  ...,  1.2988e-01,
           1.9194e-01,  9.9463e-02],
         [-1.5395e-01, -1.2753e-01,  3.9934e-01,  ...,  1.5524e-01,
           2.1006e-01,  1.0201e-01]],

        [[ 8.1586e-02, -2.7385e-01,  3.8123e-01,  ...,  4.7080e-02,
           2.3251e-01,  1.4431e-01],
         [-3.3494e-02,  5.3905e-02,  5.1104e-01,  ...,  1.4085e-01,
           1.1064e-01,  1.6878e-01],
         [-3.8856e-02, -2.1331e-01,  4.3943e-01,  ...,  1.2741e-01,
           3.7013e-01,  1.8242e-01]],

        [[ 5.0107e-02, -2.3470e-01,  4.5340e-01,  ..., -3.2213e-04,
           2.1187e-01,  1.2378e-01],
         [-3.1714e-02,  6.7890e-02,  4.9021e-01,  ...,  1.8232e-01,
           9.6012e-02,  1.7080e-01],
         [-1.4558e-01, -1.5343e-01,  4.8996e-01,  ...,  6.6474e-02,
           1.3925e-01,  1.5394e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:doc attn size torch.Size([5, 3, 128])
INFO:root:inter doc attn tensor([[[ 8.3869e-02,  1.3085e-01, -4.8617e-02,  ...,  9.2632e-03,
          -8.3175e-02, -5.9094e-02],
         [ 4.5551e-02,  1.3340e-01, -9.9578e-02,  ...,  3.9060e-02,
          -1.0990e-01, -1.4668e-01],
         [ 7.8981e-02,  1.4829e-01, -4.7768e-02,  ...,  3.5848e-02,
          -1.0756e-01, -4.7859e-02]],

        [[ 9.8442e-02,  1.6457e-01, -5.0276e-02,  ...,  1.7652e-02,
          -1.0621e-01, -5.8166e-02],
         [ 4.8879e-02,  1.4951e-01, -1.0843e-01,  ...,  3.1620e-02,
          -1.2735e-01, -1.6145e-01],
         [ 7.8612e-02,  1.6271e-01, -4.5136e-02,  ...,  2.5949e-02,
          -1.1771e-01, -7.2318e-02]],

        [[ 9.4254e-02,  1.4578e-01, -4.4311e-02,  ...,  3.1611e-03,
          -8.0346e-02, -5.1597e-02],
         [ 5.0931e-02,  1.3462e-01, -8.7846e-02,  ...,  2.8978e-02,
          -1.0367e-01, -1.2505e-01],
         [ 1.2115e-01,  1.7235e-01, -4.0241e-02,  ...,  2.2882e-02,
          -1.1064e-01, -9.2986e-02]],

        [[ 1.0250e-01,  1.5202e-01, -5.0239e-02,  ..., -1.5209e-04,
          -7.3402e-02, -5.4759e-02],
         [ 3.8737e-02,  1.4754e-01, -9.8466e-02,  ...,  4.1586e-02,
          -1.2368e-01, -1.6090e-01],
         [ 9.0104e-02,  1.5753e-01, -3.7889e-02,  ...,  2.1933e-02,
          -1.0286e-01, -6.4811e-02]],

        [[ 9.5243e-02,  1.5835e-01, -4.9917e-02,  ...,  1.5156e-02,
          -1.0402e-01, -6.2823e-02],
         [ 2.7347e-02,  1.4687e-01, -1.0567e-01,  ...,  4.0115e-02,
          -1.2375e-01, -1.3244e-01],
         [ 1.0085e-01,  1.7895e-01, -5.4719e-02,  ...,  3.3305e-02,
          -1.2235e-01, -8.3979e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:mmr!
INFO:root:query size before linear: torch.Size([256, 3, 128])
INFO:root:doc emb torch.Size([5, 3, 128])
WARNING:root:As of 7-8, current issue is when to mask query and when to mask document too
INFO:root:query after linear torch.Size([3, 256, 128]) 
INFO:root:query_mask torch.Size([3, 256])
INFO:root:query_mask tensor([[False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True],
        [False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True],
        [False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True]], device='cuda:0')
INFO:root:query attn after linear: torch.Size([3, 256, 1])
INFO:root:doc emb torch.Size([3, 5, 128])
INFO:root:query doc linear torch.Size([3, 256, 5])
INFO:root:weighted query doc linear torch.Size([3, 256, 5])
INFO:root:tensor([[[-0.0154, -0.0164, -0.0177, -0.0164, -0.0164],
         [-0.0122, -0.0117, -0.0141, -0.0134, -0.0120],
         [-0.0038, -0.0024, -0.0040, -0.0022, -0.0031],
         ...,
         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],
         [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],

        [[-0.0071, -0.0066, -0.0068, -0.0047, -0.0058],
         [-0.0144, -0.0137, -0.0132, -0.0135, -0.0128],
         [-0.0056, -0.0058, -0.0050, -0.0045, -0.0033],
         ...,
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],
         [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000]],

        [[-0.0015, -0.0016, -0.0018, -0.0016, -0.0011],
         [-0.0080, -0.0089, -0.0063, -0.0091, -0.0069],
         [-0.0145, -0.0159, -0.0182, -0.0166, -0.0161],
         ...,
         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],
         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],
         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]], device='cuda:0',
       grad_fn=<MulBackward0>)
INFO:root:first sim: torch.Size([3, 5])
INFO:root:tensor([[-0.0108,  0.0198,  0.0368,  0.0442,  0.0319],
        [-0.0670, -0.0195, -0.0251, -0.0324,  0.0049],
        [-0.0918, -0.1396, -0.0688, -0.1215, -0.0865]], device='cuda:0',
       grad_fn=<SumBackward1>)
INFO:root:doc weights torch.Size([3, 5, 128])
INFO:root:doc weights tensor([[[ 0.1160,  0.0898, -0.0373,  ..., -0.0571,  0.0322, -0.1150],
         [ 0.1195,  0.0729, -0.0351,  ..., -0.0638,  0.0343, -0.1119],
         [ 0.1010,  0.0727, -0.0251,  ..., -0.0653,  0.0349, -0.1203],
         [ 0.0989,  0.0709, -0.0272,  ..., -0.0701,  0.0404, -0.1199],
         [ 0.1035,  0.0721, -0.0321,  ..., -0.0647,  0.0356, -0.1184]],

        [[ 0.0572,  0.1346, -0.0118,  ..., -0.0541,  0.0318, -0.1179],
         [ 0.0413,  0.1239, -0.0011,  ..., -0.0614,  0.0337, -0.1217],
         [ 0.0596,  0.1199, -0.0035,  ..., -0.0536,  0.0306, -0.1085],
         [ 0.0720,  0.1379, -0.0080,  ..., -0.0496,  0.0331, -0.1112],
         [ 0.0454,  0.1149,  0.0061,  ..., -0.0656,  0.0388, -0.0931]],

        [[ 0.0832,  0.0906, -0.0110,  ..., -0.0731,  0.0336, -0.0833],
         [ 0.0913,  0.1105, -0.0335,  ..., -0.0739,  0.0298, -0.1071],
         [ 0.1114,  0.1145, -0.0292,  ..., -0.0674,  0.0286, -0.1146],
         [ 0.1007,  0.1117, -0.0349,  ..., -0.0729,  0.0331, -0.1051],
         [ 0.1099,  0.1114, -0.0353,  ..., -0.0735,  0.0282, -0.1040]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:sim 2 torch.Size([3, 5, 5])
INFO:root:sim 2 tensor([[[0.7017, 0.7495, 0.7073, 0.7120, 0.7295],
         [0.7495, 0.8201, 0.7732, 0.7768, 0.7995],
         [0.7073, 0.7732, 0.7357, 0.7375, 0.7560],
         [0.7120, 0.7768, 0.7375, 0.7423, 0.7591],
         [0.7295, 0.7995, 0.7560, 0.7591, 0.7827]],

        [[0.7491, 0.7731, 0.7266, 0.7754, 0.7430],
         [0.7731, 0.8047, 0.7523, 0.8011, 0.7758],
         [0.7266, 0.7523, 0.7131, 0.7570, 0.7298],
         [0.7754, 0.8011, 0.7570, 0.8116, 0.7746],
         [0.7430, 0.7758, 0.7298, 0.7746, 0.7607]],

        [[0.7340, 0.7427, 0.7900, 0.7378, 0.7938],
         [0.7427, 0.7789, 0.8272, 0.7729, 0.8272],
         [0.7900, 0.8272, 0.8943, 0.8229, 0.8857],
         [0.7378, 0.7729, 0.8229, 0.7690, 0.8222],
         [0.7938, 0.8272, 0.8857, 0.8222, 0.8851]]], device='cuda:0',
       grad_fn=<BmmBackward>)
INFO:root:mmr_scores torch.Size([3, 5, 5])
INFO:root:mmr_scores tensor([[[-0.3562, -0.3801, -0.3590, -0.3614, -0.3701],
         [-0.3648, -0.4001, -0.3767, -0.3785, -0.3899],
         [-0.3352, -0.3682, -0.3494, -0.3503, -0.3596],
         [-0.3339, -0.3663, -0.3467, -0.3491, -0.3575],
         [-0.3488, -0.3838, -0.3621, -0.3636, -0.3754]],

        [[-0.4081, -0.4201, -0.3968, -0.4212, -0.4050],
         [-0.3963, -0.4121, -0.3859, -0.4103, -0.3976],
         [-0.3759, -0.3887, -0.3691, -0.3911, -0.3774],
         [-0.4039, -0.4168, -0.3947, -0.4220, -0.4035],
         [-0.3691, -0.3854, -0.3625, -0.3848, -0.3779]],

        [[-0.4129, -0.4172, -0.4409, -0.4148, -0.4428],
         [-0.4412, -0.4592, -0.4834, -0.4562, -0.4834],
         [-0.4294, -0.4480, -0.4816, -0.4459, -0.4772],
         [-0.4297, -0.4472, -0.4722, -0.4453, -0.4719],
         [-0.4402, -0.4569, -0.4861, -0.4544, -0.4858]]], device='cuda:0',
       grad_fn=<SubBackward0>)
INFO:root:mmr_scores torch.Size([3, 5])
INFO:root:tensor([[[ 8.3869e-02,  1.3085e-01, -4.8617e-02,  ...,  9.2632e-03,
          -8.3175e-02, -5.9094e-02],
         [ 9.8442e-02,  1.6457e-01, -5.0276e-02,  ...,  1.7652e-02,
          -1.0621e-01, -5.8166e-02],
         [ 9.4254e-02,  1.4578e-01, -4.4311e-02,  ...,  3.1611e-03,
          -8.0346e-02, -5.1597e-02],
         [ 1.0250e-01,  1.5202e-01, -5.0239e-02,  ..., -1.5209e-04,
          -7.3402e-02, -5.4759e-02],
         [ 9.5243e-02,  1.5835e-01, -4.9917e-02,  ...,  1.5156e-02,
          -1.0402e-01, -6.2823e-02]],

        [[ 4.5551e-02,  1.3340e-01, -9.9578e-02,  ...,  3.9060e-02,
          -1.0990e-01, -1.4668e-01],
         [ 4.8879e-02,  1.4951e-01, -1.0843e-01,  ...,  3.1620e-02,
          -1.2735e-01, -1.6145e-01],
         [ 5.0931e-02,  1.3462e-01, -8.7846e-02,  ...,  2.8978e-02,
          -1.0367e-01, -1.2505e-01],
         [ 3.8737e-02,  1.4754e-01, -9.8466e-02,  ...,  4.1586e-02,
          -1.2368e-01, -1.6090e-01],
         [ 2.7347e-02,  1.4687e-01, -1.0567e-01,  ...,  4.0115e-02,
          -1.2375e-01, -1.3244e-01]],

        [[ 7.8981e-02,  1.4829e-01, -4.7768e-02,  ...,  3.5848e-02,
          -1.0756e-01, -4.7859e-02],
         [ 7.8612e-02,  1.6271e-01, -4.5136e-02,  ...,  2.5949e-02,
          -1.1771e-01, -7.2318e-02],
         [ 1.2115e-01,  1.7235e-01, -4.0241e-02,  ...,  2.2882e-02,
          -1.1064e-01, -9.2986e-02],
         [ 9.0104e-02,  1.5753e-01, -3.7889e-02,  ...,  2.1933e-02,
          -1.0286e-01, -6.4811e-02],
         [ 1.0085e-01,  1.7895e-01, -5.4719e-02,  ...,  3.3305e-02,
          -1.2235e-01, -8.3979e-02]]], device='cuda:0',
       grad_fn=<TransposeBackward0>)
INFO:root:tensor([[[ 1.6927e-02,  2.6409e-02, -9.8123e-03,  ...,  1.8696e-03,
          -1.6787e-02, -1.1927e-02],
         [ 2.0028e-02,  3.3483e-02, -1.0229e-02,  ...,  3.5913e-03,
          -2.1609e-02, -1.1834e-02],
         [ 1.9179e-02,  2.9663e-02, -9.0165e-03,  ...,  6.4323e-04,
          -1.6349e-02, -1.0499e-02],
         [ 2.0846e-02,  3.0916e-02, -1.0217e-02,  ..., -3.0931e-05,
          -1.4928e-02, -1.1136e-02],
         [ 1.9392e-02,  3.2240e-02, -1.0163e-02,  ...,  3.0859e-03,
          -2.1178e-02, -1.2791e-02]],

        [[ 9.2329e-03,  2.7039e-02, -2.0184e-02,  ...,  7.9172e-03,
          -2.2276e-02, -2.9732e-02],
         [ 9.9187e-03,  3.0339e-02, -2.2003e-02,  ...,  6.4164e-03,
          -2.5842e-02, -3.2762e-02],
         [ 1.0302e-02,  2.7229e-02, -1.7768e-02,  ...,  5.8614e-03,
          -2.0970e-02, -2.5294e-02],
         [ 7.8521e-03,  2.9907e-02, -1.9959e-02,  ...,  8.4296e-03,
          -2.5070e-02, -3.2614e-02],
         [ 5.5434e-03,  2.9771e-02, -2.1420e-02,  ...,  8.1317e-03,
          -2.5086e-02, -2.6848e-02]],

        [[ 1.5999e-02,  3.0040e-02, -9.6763e-03,  ...,  7.2618e-03,
          -2.1789e-02, -9.6948e-03],
         [ 1.6095e-02,  3.3313e-02, -9.2410e-03,  ...,  5.3126e-03,
          -2.4099e-02, -1.4806e-02],
         [ 2.4889e-02,  3.5406e-02, -8.2668e-03,  ...,  4.7008e-03,
          -2.2729e-02, -1.9102e-02],
         [ 1.8448e-02,  3.2254e-02, -7.7575e-03,  ...,  4.4906e-03,
          -2.1060e-02, -1.3270e-02],
         [ 2.0667e-02,  3.6671e-02, -1.1213e-02,  ...,  6.8250e-03,
          -2.5072e-02, -1.7209e-02]]], device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc_emb torch.Size([3, 5, 128])
INFO:root:doc mmr weighted size torch.Size([5, 3, 128])
INFO:root:input for adding context torch.Size([256, 5, 3, 128])
DEBUG:root:local doc emb size after global enc torch.Size([256, 15, 128]):
DEBUG:root:global doc emb size after global_enc torch.Size([5, 3, 128]):
DEBUG:root:query emb size after global_enc torch.Size([256, 3, 128]):
DEBUG:root:global doc emb size after encoder torch.Size([5, 3, 128]):
DEBUG:root:local doc emb size after encoder torch.Size([256, 15, 128]):
INFO:root:tgt attn mask torch.Size([256, 256])
WARNING:root:Use global or local representations for decoder?
DEBUG:root:output after final linear layer: torch.Size([256, 3, 50265])
DEBUG:root:output torch.Size([3, 50265, 256])
INFO:root:tensor([[[ 0.9374, -0.1508, -0.9061,  ...,  0.1437,  0.0777,  0.7110],
         [-0.1922,  0.5505,  0.2471,  ...,  3.6077,  3.6007,  3.6235],
         [ 2.9946, -0.5734, -1.3418,  ...,  0.7428,  0.1462,  0.2859],
         ...,
         [ 0.5856,  0.1150,  0.5595,  ...,  0.5060,  0.0490,  0.5078],
         [ 0.9344,  0.7755, -0.2401,  ...,  0.8598,  0.5881,  0.5847],
         [ 1.2553,  0.8633,  1.1828,  ...,  0.1572, -0.5613, -0.0832]],

        [[ 0.1090,  0.2922, -0.2941,  ..., -0.0181,  0.1812,  0.7083],
         [ 0.6492,  0.2687, -0.7894,  ...,  3.9296,  3.6712,  3.5461],
         [ 3.8621,  0.1547, -0.8502,  ...,  0.3226,  0.5274,  0.3578],
         ...,
         [ 0.0043,  0.2706,  0.7281,  ..., -0.0910, -0.1428, -0.0094],
         [ 1.0951,  1.1537,  0.4505,  ...,  1.0090,  0.9151,  0.8856],
         [ 0.7265,  0.6202,  0.6214,  ..., -0.3869, -0.4184, -0.3349]],

        [[ 0.5910, -0.0300,  0.3915,  ...,  0.2451,  0.0816,  0.3603],
         [-0.6369,  0.2944,  0.5775,  ...,  3.6070,  3.5886,  3.7998],
         [ 3.5644, -0.2289, -0.4389,  ...,  0.4553,  0.6705,  0.4498],
         ...,
         [ 0.5752,  0.5056,  0.7874,  ...,  0.2597,  0.0652,  0.1809],
         [ 0.7269,  1.1483,  1.1570,  ...,  1.0559,  1.1996,  1.4683],
         [ 1.4429, -0.0306,  0.6201,  ..., -0.5002, -0.2725, -0.0079]]],
       device='cuda:0', grad_fn=<PermuteBackward>)
INFO:root:Batch # 0
WARNING:root:FORCED EXIT!
