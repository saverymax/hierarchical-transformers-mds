DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/facebook/bart-large/config.json HTTP/1.1" 200 0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json from cache at /lscratch/60779897/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.40bd49bcec9d93d8b0bfbd020088e2e1b6e6bb03e8e80aa5144638f90ca6bd61
INFO:transformers.configuration_utils:Model config BartConfig {
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForMaskedLM",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "normalize_before": false,
  "normalize_embedding": true,
  "num_hidden_layers": 12,
  "output_past": false,
  "pad_token_id": 1,
  "prefix": " ",
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "no_repeat_ngram_size": 3,
      "num_beams": 4
    }
  },
  "vocab_size": 50265
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-large-vocab.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-large-merges.txt HTTP/1.1" 200 0
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /lscratch/60779897/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /lscratch/60779897/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
INFO:root:EOS token from facebook/bart-large tokenizer </s>, 2
INFO:root:Hierarchical config: HierarchicalTransformerConfig {
  "batch_size": 3,
  "bos_token": "</s>",
  "dec_hidden_dim": 128,
  "decoder_attn_mask": true,
  "decoder_layers": 2,
  "decoder_type": "transformer",
  "dropout": 0.1,
  "enc_hidden_dim": 128,
  "eos_token": "</s>",
  "ffw_dim": 1024,
  "global_enc_layers": 2,
  "hf_model": null,
  "init_bert_weights": false,
  "is_encoder_decoder": true,
  "k_docs": null,
  "local_enc_layers": 2,
  "max_docs": 5,
  "max_seq_len": 32,
  "mmr": true,
  "model_type": "hier_transformer",
  "multi_head_pooling": true,
  "n_att_heads": 4,
  "padding_mask": true,
  "query_doc_attn": false,
  "use_cls_token": false,
  "vocab_size": 50265
}

WARNING:root:Register buffer?
INFO:root:Initializing random embeddings
INFO:root:Using cuda:0
INFO:root:Tasks provided: ['mediqa']
INFO:root:Number of all articles: 552
INFO:root:Beginning training for 1 epochs, batch size 3
INFO:root:source mask: <class 'torch.Tensor'>
INFO:root:torch.Size([3, 32])
INFO:root:Encoded query: tensor([41552,   565,  2336,   530, 15698, 22718, 30018,   250,    12,  4688,
          104, 28696, 46392,  7744, 15698,    99,    16,     5,  1303,     9],
       device='cuda:0')
INFO:root:Decoded query: <TASK> MEDIQA-AnS <QUESTION> what is the cause of
INFO:root:Encoded source: tensor([42337,   225,  8632,  6924,  7858,  9041,    29,    36,  4054,  3048,
         1723,  3256,   318,   110,   920,    34,    10, 36764,  8632,  1144],
       device='cuda:0')
INFO:root:Decoded source: Congenital Heart Defects (CAUSES): If your child has a congenital heart
INFO:root:Encoded decoder input:
tensor([  148, 38593,  9764,     9, 36764,  8632,  1144, 23487,    32,  4727,
            4,    20,   810,  2433,    13,   209,  1272,    14,  1437,  5948],
       device='cuda:0')
INFO:root:Decoded shifted summary:
 duringCauses of congenital heart defects are unknown. The risk factors for these problems that  occur
INFO:root:doc_mask size torch.Size([3, 5])
INFO:root:doc_mask tensor([[True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True]], device='cuda:0')
INFO:root:TOken mask! torch.Size([3, 5, 32])
INFO:root:query mask! torch.Size([3, 32])
INFO:root:target mask! torch.Size([3, 32])
DEBUG:root:dob emb size for input: torch.Size([3, 5, 32, 128])
DEBUG:root:tgt emb size: torch.Size([32, 3, 128])
DEBUG:root:query emb size: torch.Size([32, 3, 128])
DEBUG:root:dob emb size for input after initial reshape: torch.Size([32, 15, 128])
DEBUG:root:token mask emb size for input after initial reshape: torch.Size([15, 32])
INFO:root:initial doc emb tensor([[[-0.7256,  0.3899,  1.0028,  ..., -0.7633, -0.3954, -0.7807],
         [-0.7256,  0.3899,  1.0028,  ..., -0.7633, -0.3954, -0.7807],
         [ 0.8076, -0.0225, -0.4945,  ...,  0.7353,  0.5867,  0.9437],
         ...,
         [-0.2514, -0.8421, -0.5919,  ..., -0.3809,  0.3636, -1.0331],
         [-0.2514, -0.8421, -0.5919,  ..., -0.3809,  0.3636, -1.0331],
         [ 1.0441, -1.0242, -0.7165,  ...,  1.1091,  0.0197,  0.7977]],

        [[-0.3773,  0.8319,  0.6291,  ..., -0.7775,  1.0740, -0.3598],
         [-0.3773,  0.8319,  0.6291,  ..., -0.7775,  1.0740, -0.3598],
         [ 0.6703, -0.8036,  1.0297,  ...,  1.0708,  0.6572,  0.2901],
         ...,
         [-0.7702,  0.0313,  0.9694,  ...,  0.9958, -0.5110,  0.0438],
         [-0.7702,  0.0313,  0.9694,  ...,  0.9958, -0.5110,  0.0438],
         [ 0.3688, -0.8436, -0.3485,  ..., -0.8590,  0.6861,  1.0235]],

        [[-0.7747,  1.0078,  0.6748,  ..., -0.3200, -1.0599,  0.0548],
         [-0.7747,  1.0078,  0.6748,  ..., -0.3200, -1.0599,  0.0548],
         [ 0.1660,  0.8209, -0.5304,  ..., -0.1108,  0.3044,  0.5683],
         ...,
         [ 1.0123, -0.9462,  0.6756,  ...,  0.2861, -0.9047,  0.0892],
         [ 1.0123, -0.9462,  0.6756,  ...,  0.2861, -0.9047,  0.0892],
         [-0.9238, -0.4348,  0.7153,  ..., -0.6488,  0.1248, -0.7707]],

        ...,

        [[-0.9295, -0.8705,  0.8429,  ..., -0.8499, -1.0094,  0.8390],
         [ 0.4575, -0.6221, -0.5336,  ...,  0.8837,  0.6438,  0.4584],
         [-0.6393,  0.3903,  0.5578,  ...,  0.0746,  0.1325,  0.9456],
         ...,
         [ 0.6527,  0.9559,  0.9509,  ...,  0.7820, -0.7670, -0.0734],
         [ 0.2206, -0.2345, -0.4383,  ..., -0.2133,  0.8865, -0.6897],
         [ 0.8518,  0.3667, -0.5997,  ...,  0.1720, -0.5495, -0.1025]],

        [[ 0.8097,  0.0317, -0.0047,  ..., -0.5198, -0.6784,  0.6780],
         [ 0.7490,  0.3502,  0.7130,  ..., -0.1472, -0.1239, -0.9945],
         [-0.2447,  1.0269,  0.4270,  ...,  1.1222,  0.2635,  0.6544],
         ...,
         [ 0.0635, -0.5398,  0.6856,  ...,  0.1468, -0.9154,  0.3627],
         [ 0.6527,  0.9559,  0.9509,  ...,  0.7820, -0.7670, -0.0734],
         [ 0.3277, -0.9927,  0.2269,  ..., -0.0090, -0.8984,  0.0174]],

        [[ 0.0475,  0.2962,  0.7085,  ...,  0.3074,  0.9282, -0.8090],
         [-0.5747, -0.2957,  0.1382,  ...,  0.6747,  0.9480,  0.6746],
         [-0.5231, -0.7860, -0.2560,  ...,  0.8273, -0.7343,  1.1123],
         ...,
         [ 0.2005,  1.0858, -0.2951,  ...,  1.1191, -0.2884, -0.0319],
         [-0.2436,  0.4272, -0.2052,  ..., -0.7657,  0.3080, -0.1375],
         [ 0.2998,  0.8655,  1.0313,  ...,  0.6519,  1.0633,  0.2947]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:src mask tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True]], device='cuda:0')
DEBUG:root:layer type: local
INFO:root:local layer emb: tensor([[[-0.1324,  1.3524,  0.6316,  ..., -0.5008, -0.0812, -0.3168],
         [-1.1374,  0.9996, -0.3331,  ..., -0.9295, -0.6577, -0.2194],
         [ 0.8948,  0.7506, -0.6288,  ...,  1.7395,  1.2104, -0.9668],
         ...,
         [-0.3600,  0.3390, -0.8981,  ...,  0.3455,  0.3249,  0.4354],
         [ 0.5015, -0.2932, -0.6143,  ...,  0.3953,  0.5759, -0.4702],
         [ 0.9719, -0.8693, -0.7240,  ...,  2.0423, -0.0057,  1.7011]],

        [[-0.4119,  1.2992,  1.2095,  ..., -0.1571,  0.0085, -0.5661],
         [-0.2816,  1.4071,  1.4188,  ...,  0.1273,  1.1798,  0.2237],
         [ 2.0105, -0.4999,  1.5234,  ...,  1.5684,  0.8447,  0.9228],
         ...,
         [-0.1022, -0.0272,  1.5163,  ..., -0.3115,  0.1282,  1.0814],
         [ 0.1903,  0.2217,  2.1623,  ...,  1.5120, -0.2367,  0.5854],
         [ 1.2688, -0.7363,  0.2778,  ..., -0.4690,  1.2250, -0.5946]],

        [[ 0.9141, -0.3508,  1.5426,  ...,  0.3040, -1.0831,  0.5620],
         [-0.0115,  0.6094,  1.1767,  ...,  0.1190, -0.5282,  0.4989],
         [ 1.5890, -0.2211,  0.5911,  ...,  0.5898, -0.0987,  0.6987],
         ...,
         [-0.0320, -1.4018,  1.5228,  ...,  1.4625, -1.2480,  1.0189],
         [ 0.1855, -1.9702,  1.8970,  ...,  1.1305, -1.4670,  1.1798],
         [ 0.0505, -1.6117,  1.0938,  ..., -0.0952, -0.1664,  0.1443]],

        ...,

        [[-1.7847, -1.5097,  0.4979,  ...,  0.0858, -0.0058,  1.7772],
         [-0.4741, -1.7072, -0.4800,  ...,  2.4289,  1.5896,  0.9395],
         [-1.3007,  0.2744,  0.6273,  ...,  2.1414,  0.7556,  1.5661],
         ...,
         [-0.4768, -0.0825,  0.3397,  ...,  1.4911, -0.4083,  0.5381],
         [-0.6338, -1.7072, -0.2016,  ...,  1.5243,  1.1696,  0.3890],
         [ 0.0356, -0.4865, -0.6146,  ...,  1.7655,  0.9721,  0.4816]],

        [[-0.0301,  0.3482, -0.2293,  ...,  0.7407, -0.4025, -0.0162],
         [-0.5215,  0.2426,  1.4479,  ...,  0.7295,  0.8286,  0.4301],
         [-1.2641,  1.3296,  0.6218,  ...,  2.6324,  0.7447,  1.6899],
         ...,
         [ 0.0235, -0.7305,  1.4551,  ...,  1.6317, -0.6542,  1.0836],
         [-0.2406,  0.6015,  1.0590,  ...,  1.4609, -0.4911, -0.3892],
         [-0.0168, -1.4280, -0.4659,  ...,  0.8613,  0.2536,  0.7852]],

        [[-0.2759,  0.8203,  1.9731,  ...,  1.8902,  1.4626,  0.5150],
         [-1.7284, -0.1456,  1.1240,  ...,  1.6036,  0.9920,  2.2454],
         [-1.1282, -0.2260,  0.1399,  ...,  2.0915, -0.8167,  1.8088],
         ...,
         [ 0.1213,  1.7192,  0.5155,  ..., -0.1036, -0.1988,  2.1732],
         [-0.8404,  0.0733,  0.5280,  ..., -0.0077,  0.2483,  0.6831],
         [-0.3027,  1.7069,  1.2941,  ...,  1.3301,  0.4697,  1.4812]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([32, 15, 128]):
INFO:root:Using global transformer
INFO:root:initial input tensor([[[-0.1324,  1.3524,  0.6316,  ..., -0.5008, -0.0812, -0.3168],
         [-1.1374,  0.9996, -0.3331,  ..., -0.9295, -0.6577, -0.2194],
         [ 0.8948,  0.7506, -0.6288,  ...,  1.7395,  1.2104, -0.9668],
         ...,
         [-0.3600,  0.3390, -0.8981,  ...,  0.3455,  0.3249,  0.4354],
         [ 0.5015, -0.2932, -0.6143,  ...,  0.3953,  0.5759, -0.4702],
         [ 0.9719, -0.8693, -0.7240,  ...,  2.0423, -0.0057,  1.7011]],

        [[-0.4119,  1.2992,  1.2095,  ..., -0.1571,  0.0085, -0.5661],
         [-0.2816,  1.4071,  1.4188,  ...,  0.1273,  1.1798,  0.2237],
         [ 2.0105, -0.4999,  1.5234,  ...,  1.5684,  0.8447,  0.9228],
         ...,
         [-0.1022, -0.0272,  1.5163,  ..., -0.3115,  0.1282,  1.0814],
         [ 0.1903,  0.2217,  2.1623,  ...,  1.5120, -0.2367,  0.5854],
         [ 1.2688, -0.7363,  0.2778,  ..., -0.4690,  1.2250, -0.5946]],

        [[ 0.9141, -0.3508,  1.5426,  ...,  0.3040, -1.0831,  0.5620],
         [-0.0115,  0.6094,  1.1767,  ...,  0.1190, -0.5282,  0.4989],
         [ 1.5890, -0.2211,  0.5911,  ...,  0.5898, -0.0987,  0.6987],
         ...,
         [-0.0320, -1.4018,  1.5228,  ...,  1.4625, -1.2480,  1.0189],
         [ 0.1855, -1.9702,  1.8970,  ...,  1.1305, -1.4670,  1.1798],
         [ 0.0505, -1.6117,  1.0938,  ..., -0.0952, -0.1664,  0.1443]],

        ...,

        [[-1.7847, -1.5097,  0.4979,  ...,  0.0858, -0.0058,  1.7772],
         [-0.4741, -1.7072, -0.4800,  ...,  2.4289,  1.5896,  0.9395],
         [-1.3007,  0.2744,  0.6273,  ...,  2.1414,  0.7556,  1.5661],
         ...,
         [-0.4768, -0.0825,  0.3397,  ...,  1.4911, -0.4083,  0.5381],
         [-0.6338, -1.7072, -0.2016,  ...,  1.5243,  1.1696,  0.3890],
         [ 0.0356, -0.4865, -0.6146,  ...,  1.7655,  0.9721,  0.4816]],

        [[-0.0301,  0.3482, -0.2293,  ...,  0.7407, -0.4025, -0.0162],
         [-0.5215,  0.2426,  1.4479,  ...,  0.7295,  0.8286,  0.4301],
         [-1.2641,  1.3296,  0.6218,  ...,  2.6324,  0.7447,  1.6899],
         ...,
         [ 0.0235, -0.7305,  1.4551,  ...,  1.6317, -0.6542,  1.0836],
         [-0.2406,  0.6015,  1.0590,  ...,  1.4609, -0.4911, -0.3892],
         [-0.0168, -1.4280, -0.4659,  ...,  0.8613,  0.2536,  0.7852]],

        [[-0.2759,  0.8203,  1.9731,  ...,  1.8902,  1.4626,  0.5150],
         [-1.7284, -0.1456,  1.1240,  ...,  1.6036,  0.9920,  2.2454],
         [-1.1282, -0.2260,  0.1399,  ...,  2.0915, -0.8167,  1.8088],
         ...,
         [ 0.1213,  1.7192,  0.5155,  ..., -0.1036, -0.1988,  2.1732],
         [-0.8404,  0.0733,  0.5280,  ..., -0.0077,  0.2483,  0.6831],
         [-0.3027,  1.7069,  1.2941,  ...,  1.3301,  0.4697,  1.4812]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
INFO:root:input_size torch.Size([32, 15, 128])
INFO:root:query size torch.Size([32, 3, 128])
INFO:root:src padding_mask size torch.Size([15, 32])
INFO:root:Computing multi-head pooling
INFO:root:tensor([[[-0.1324,  1.3524,  0.6316,  ..., -0.5008, -0.0812, -0.3168],
         [-1.1374,  0.9996, -0.3331,  ..., -0.9295, -0.6577, -0.2194],
         [ 0.8948,  0.7506, -0.6288,  ...,  1.7395,  1.2104, -0.9668],
         ...,
         [-0.3600,  0.3390, -0.8981,  ...,  0.3455,  0.3249,  0.4354],
         [ 0.5015, -0.2932, -0.6143,  ...,  0.3953,  0.5759, -0.4702],
         [ 0.9719, -0.8693, -0.7240,  ...,  2.0423, -0.0057,  1.7011]],

        [[-0.4119,  1.2992,  1.2095,  ..., -0.1571,  0.0085, -0.5661],
         [-0.2816,  1.4071,  1.4188,  ...,  0.1273,  1.1798,  0.2237],
         [ 2.0105, -0.4999,  1.5234,  ...,  1.5684,  0.8447,  0.9228],
         ...,
         [-0.1022, -0.0272,  1.5163,  ..., -0.3115,  0.1282,  1.0814],
         [ 0.1903,  0.2217,  2.1623,  ...,  1.5120, -0.2367,  0.5854],
         [ 1.2688, -0.7363,  0.2778,  ..., -0.4690,  1.2250, -0.5946]],

        [[ 0.9141, -0.3508,  1.5426,  ...,  0.3040, -1.0831,  0.5620],
         [-0.0115,  0.6094,  1.1767,  ...,  0.1190, -0.5282,  0.4989],
         [ 1.5890, -0.2211,  0.5911,  ...,  0.5898, -0.0987,  0.6987],
         ...,
         [-0.0320, -1.4018,  1.5228,  ...,  1.4625, -1.2480,  1.0189],
         [ 0.1855, -1.9702,  1.8970,  ...,  1.1305, -1.4670,  1.1798],
         [ 0.0505, -1.6117,  1.0938,  ..., -0.0952, -0.1664,  0.1443]],

        ...,

        [[-1.7847, -1.5097,  0.4979,  ...,  0.0858, -0.0058,  1.7772],
         [-0.4741, -1.7072, -0.4800,  ...,  2.4289,  1.5896,  0.9395],
         [-1.3007,  0.2744,  0.6273,  ...,  2.1414,  0.7556,  1.5661],
         ...,
         [-0.4768, -0.0825,  0.3397,  ...,  1.4911, -0.4083,  0.5381],
         [-0.6338, -1.7072, -0.2016,  ...,  1.5243,  1.1696,  0.3890],
         [ 0.0356, -0.4865, -0.6146,  ...,  1.7655,  0.9721,  0.4816]],

        [[-0.0301,  0.3482, -0.2293,  ...,  0.7407, -0.4025, -0.0162],
         [-0.5215,  0.2426,  1.4479,  ...,  0.7295,  0.8286,  0.4301],
         [-1.2641,  1.3296,  0.6218,  ...,  2.6324,  0.7447,  1.6899],
         ...,
         [ 0.0235, -0.7305,  1.4551,  ...,  1.6317, -0.6542,  1.0836],
         [-0.2406,  0.6015,  1.0590,  ...,  1.4609, -0.4911, -0.3892],
         [-0.0168, -1.4280, -0.4659,  ...,  0.8613,  0.2536,  0.7852]],

        [[-0.2759,  0.8203,  1.9731,  ...,  1.8902,  1.4626,  0.5150],
         [-1.7284, -0.1456,  1.1240,  ...,  1.6036,  0.9920,  2.2454],
         [-1.1282, -0.2260,  0.1399,  ...,  2.0915, -0.8167,  1.8088],
         ...,
         [ 0.1213,  1.7192,  0.5155,  ..., -0.1036, -0.1988,  2.1732],
         [-0.8404,  0.0733,  0.5280,  ..., -0.0077,  0.2483,  0.6831],
         [-0.3027,  1.7069,  1.2941,  ...,  1.3301,  0.4697,  1.4812]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)
INFO:root:attn scores torch.Size([32, 15, 4])
INFO:root:values torch.Size([32, 15, 128])
WARNING:root:Fully masked documents will result in nan
INFO:root:scores torch.Size([32, 60, 1])
INFO:root:values after shaping torch.Size([32, 60, 32])
INFO:root:tensor([[[ 0.3191, -0.2374,  0.9333,  ...,  0.2691,  0.6567,  0.3279],
         [ 0.1810, -1.0992, -0.1279,  ..., -1.1896,  1.1990,  0.7427],
         [-0.2306, -0.6450,  0.7705,  ...,  0.1176, -0.1545, -0.0554],
         ...,
         [ 0.9617, -0.6677,  0.7008,  ...,  0.0296,  0.3123,  0.8174],
         [-0.0817, -0.3009, -0.0055,  ..., -0.0120,  0.2090,  0.6009],
         [ 0.5199,  1.0760,  0.6190,  ...,  0.5646, -0.0522,  0.5314]],

        [[-0.0913, -0.0295,  0.0229,  ...,  0.8606,  0.8758,  0.1040],
         [ 0.9731, -0.7204, -0.0118,  ..., -0.6143,  1.0474, -0.1989],
         [-0.5641, -0.2338,  0.0406,  ..., -0.5304, -0.8641, -0.5324],
         ...,
         [ 0.5181, -0.5760,  0.5584,  ..., -0.5070,  0.5375, -0.3840],
         [-0.3052, -1.1011,  0.9414,  ..., -0.0219,  0.3034,  0.4050],
         [ 1.1378,  0.1226, -0.3119,  ...,  0.8509,  0.0615,  0.1378]],

        [[ 0.1587, -0.6586,  0.0322,  ...,  0.5374,  0.3631,  1.0240],
         [ 0.7139, -1.6731,  0.3619,  ..., -0.3819,  1.2857, -0.3662],
         [-0.6148, -1.3209, -0.0863,  ...,  0.0752, -0.4523,  0.1490],
         ...,
         [ 0.2081, -0.6980,  0.2828,  ..., -0.3657,  0.6751,  0.8738],
         [-0.6251, -0.6099,  0.8130,  ..., -0.7033,  0.2230,  0.6598],
         [-0.1454,  0.8882, -0.1010,  ..., -0.7609,  0.1944, -0.2540]],

        ...,

        [[-0.0972, -0.8842,  0.0167,  ..., -0.1419,  0.3141,  0.3829],
         [-0.0948, -0.4945, -0.2650,  ...,  0.5746,  0.9146, -0.1916],
         [ 0.0502,  0.0666,  0.5574,  ..., -0.9408, -0.0509, -0.7718],
         ...,
         [-0.0430, -1.6321,  0.6401,  ..., -0.5603,  0.3038,  0.3210],
         [-0.5525,  0.5299, -0.4886,  ..., -0.8519, -0.6841, -1.1795],
         [ 0.4297,  1.6248,  0.0090,  ...,  0.5466,  1.0745, -0.3489]],

        [[-0.9936, -0.2475,  0.3803,  ...,  0.1024,  0.6231,  0.2373],
         [ 0.0447, -1.4701,  0.1655,  ...,  0.4810,  0.2158, -0.9644],
         [-0.4479, -0.2799, -0.2807,  ..., -0.8002, -0.4716, -0.3959],
         ...,
         [ 0.0720, -0.9715,  0.2213,  ...,  0.2718,  0.2362, -0.0703],
         [-0.2293, -0.1604, -0.0983,  ..., -0.4386, -0.9703, -0.5586],
         [ 0.5806,  1.1884, -0.2001,  ...,  1.3381,  0.2915,  0.1432]],

        [[-0.6499,  0.3530,  0.4618,  ...,  0.5575,  0.4083,  1.0041],
         [ 0.4759, -1.2471, -0.6575,  ...,  0.3666,  0.6468,  0.1839],
         [ 1.1287,  0.0231, -0.6769,  ..., -0.2516,  0.2260,  0.6069],
         ...,
         [ 0.8675, -1.3775,  0.0166,  ..., -0.0139,  0.8028, -0.5901],
         [-0.1717, -0.0874, -0.7016,  ..., -0.6699, -0.0732,  0.0954],
         [ 1.1223,  0.3343,  0.4723,  ...,  0.8095,  0.0433, -0.8502]]],
       device='cuda:0', grad_fn=<ViewBackward>)
INFO:root:tokens after weightin torch.Size([32, 60, 32])
INFO:root:tensor([[[ 0.3546, -0.2637,  1.0370,  ...,  0.2990,  0.7297,  0.3643],
         [ 0.2012, -1.2214, -0.1421,  ..., -1.3218,  1.3322,  0.8253],
         [-0.2562, -0.7167,  0.8562,  ...,  0.1307, -0.1717, -0.0615],
         ...,
         [ 1.0686, -0.7419,  0.7787,  ...,  0.0328,  0.3470,  0.9082],
         [-0.0907, -0.3343, -0.0062,  ..., -0.0134,  0.2322,  0.6677],
         [ 0.5777,  1.1955,  0.6878,  ...,  0.6274, -0.0580,  0.5904]],

        [[-0.1015, -0.0328,  0.0255,  ...,  0.9563,  0.9731,  0.1156],
         [ 1.0812, -0.8004, -0.0131,  ..., -0.6826,  1.1638, -0.2210],
         [-0.6268, -0.2598,  0.0451,  ..., -0.5893, -0.9601, -0.5915],
         ...,
         [ 0.5757, -0.6399,  0.6204,  ..., -0.5633,  0.5973, -0.4266],
         [-0.3391, -1.2234,  1.0460,  ..., -0.0244,  0.3371,  0.4500],
         [ 1.2642,  0.1362, -0.3465,  ...,  0.9455,  0.0683,  0.1532]],

        [[ 0.1763, -0.7318,  0.0357,  ...,  0.5971,  0.4035,  1.1377],
         [ 0.7932, -1.8590,  0.4022,  ..., -0.4244,  1.4285, -0.4069],
         [-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],
         ...,
         [ 0.2312, -0.7756,  0.3142,  ..., -0.4063,  0.7501,  0.9708],
         [-0.6946, -0.6776,  0.9034,  ..., -0.7814,  0.2478,  0.7331],
         [-0.1616,  0.9868, -0.1122,  ..., -0.8455,  0.2160, -0.2823]],

        ...,

        [[-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],
         [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],
         [ 0.0558,  0.0740,  0.6194,  ..., -1.0454, -0.0566, -0.8575],
         ...,
         [-0.0477, -1.8134,  0.7112,  ..., -0.6226,  0.3375,  0.3567],
         [-0.6139,  0.5888, -0.5429,  ..., -0.9466, -0.7601, -1.3106],
         [ 0.4774,  1.8053,  0.0100,  ...,  0.6073,  1.1939, -0.3877]],

        [[-1.1040, -0.2750,  0.4226,  ...,  0.1137,  0.6924,  0.2637],
         [ 0.0497, -1.6334,  0.1839,  ...,  0.5345,  0.2398, -1.0716],
         [-0.4977, -0.3110, -0.3119,  ..., -0.8891, -0.5240, -0.4399],
         ...,
         [ 0.0800, -1.0795,  0.2459,  ...,  0.3020,  0.2624, -0.0781],
         [-0.2548, -0.1783, -0.1093,  ..., -0.4873, -1.0781, -0.6206],
         [ 0.6451,  1.3204, -0.2223,  ...,  1.4868,  0.3239,  0.1591]],

        [[-0.7222,  0.3923,  0.5131,  ...,  0.6194,  0.4536,  1.1157],
         [ 0.5288, -1.3857, -0.7306,  ...,  0.4073,  0.7187,  0.2044],
         [ 1.2541,  0.0257, -0.7521,  ..., -0.2796,  0.2511,  0.6743],
         ...,
         [ 0.9639, -1.5306,  0.0184,  ..., -0.0154,  0.8920, -0.6556],
         [-0.1908, -0.0971, -0.7795,  ..., -0.7444, -0.0813,  0.1060],
         [ 1.2470,  0.3714,  0.5248,  ...,  0.8995,  0.0481, -0.9446]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc representation after sumtorch.Size([60, 32])
INFO:root:tensor([[-16.2927,  -3.2200,   6.5578,  ...,  15.2326,   1.0238,   9.6713],
        [ 14.7714, -32.0782,  -0.3926,  ...,  -4.3532,  26.2025,   0.2407],
        [ 11.5106,  -6.8318,   2.5498,  ...,  -9.0929,  -5.6722,   0.4944],
        ...,
        [ 10.0117, -33.5500,   6.7564,  ...,  -2.6416,  14.0953,   3.1277],
        [  6.4047,  -4.7958,  -3.3227,  ..., -11.0936,  -1.3334,   2.2753],
        [  2.7755,  19.8748,   0.4967,  ...,  13.8891,   0.0640,  -3.1877]],
       device='cuda:0', grad_fn=<SumBackward1>)
INFO:root:doc representation reshpae torch.Size([5, 3, 128])
INFO:root:doc representation torch.Size([5, 3, 128])
INFO:root:tensor([[[ 0.7164, -2.8836, 13.7662,  ...,  1.6176,  6.1996,  8.1884],
         [ 0.0552, -1.3787, 16.8205,  ..., -1.3000,  5.4559,  8.0836],
         [-2.5898, -1.9006, 14.1089,  ...,  1.1311,  2.9563,  7.1888]],

        [[-1.8091,  0.4610, 15.7990,  ..., -1.3091,  6.7274,  6.9306],
         [-0.5883, -1.9775, 16.4020,  ..., -2.9540,  6.1166,  7.6501],
         [ 1.3547, -1.9865, 15.8259,  ...,  0.5093,  5.0007, 10.9929]],

        [[ 1.7684,  0.2898, 17.0751,  ...,  0.4242,  6.0855,  9.7708],
         [-1.9198,  0.3388, 16.7500,  ...,  2.5085,  5.9161,  7.6549],
         [-3.9544, -1.4765, 14.6459,  ..., -1.0468,  5.9242,  9.3185]],

        [[ 0.0432,  0.7290, 16.1131,  ...,  0.9759,  5.4378, 11.2539],
         [-0.9932, -0.7192, 18.2861,  ..., -0.4387,  7.1894, 12.9086],
         [-3.0535, -4.3425, 18.9844,  ..., -2.9294,  6.7912, 15.2646]],

        [[ 0.8298,  1.5952, 17.3281,  ...,  1.7653,  6.1669,  9.6189],
         [-3.8898, -1.4770, 15.0745,  ..., -3.5334,  5.0094, 10.1064],
         [-2.4131, -0.7866, 15.2574,  ..., -2.6820,  6.0036,  9.1492]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:Pooling size torch.Size([5, 3, 128])
INFO:root:x pooled tensor([[[ 0.7164, -2.8836, 13.7662,  ...,  1.6176,  6.1996,  8.1884],
         [ 0.0552, -1.3787, 16.8205,  ..., -1.3000,  5.4559,  8.0836],
         [-2.5898, -1.9006, 14.1089,  ...,  1.1311,  2.9563,  7.1888]],

        [[-1.8091,  0.4610, 15.7990,  ..., -1.3091,  6.7274,  6.9306],
         [-0.5883, -1.9775, 16.4020,  ..., -2.9540,  6.1166,  7.6501],
         [ 1.3547, -1.9865, 15.8259,  ...,  0.5093,  5.0007, 10.9929]],

        [[ 1.7684,  0.2898, 17.0751,  ...,  0.4242,  6.0855,  9.7708],
         [-1.9198,  0.3388, 16.7500,  ...,  2.5085,  5.9161,  7.6549],
         [-3.9544, -1.4765, 14.6459,  ..., -1.0468,  5.9242,  9.3185]],

        [[ 0.0432,  0.7290, 16.1131,  ...,  0.9759,  5.4378, 11.2539],
         [-0.9932, -0.7192, 18.2861,  ..., -0.4387,  7.1894, 12.9086],
         [-3.0535, -4.3425, 18.9844,  ..., -2.9294,  6.7912, 15.2646]],

        [[ 0.8298,  1.5952, 17.3281,  ...,  1.7653,  6.1669,  9.6189],
         [-3.8898, -1.4770, 15.0745,  ..., -3.5334,  5.0094, 10.1064],
         [-2.4131, -0.7866, 15.2574,  ..., -2.6820,  6.0036,  9.1492]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:doc attn size torch.Size([5, 3, 128])
INFO:root:inter doc attn tensor([[[ 1.7081,  6.6247, -0.9537,  ...,  0.5410, -0.5112, -2.0923],
         [ 0.8990,  4.0860, -1.8945,  ...,  1.5545, -1.1429, -3.2370],
         [ 0.0820,  6.7393, -1.9505,  ...,  2.0692, -1.9531, -4.8317]],

        [[ 1.4835,  6.1515, -1.4281,  ...,  0.7242, -0.0603, -1.7523],
         [ 1.0401,  4.3158, -1.4345,  ...,  1.5483, -0.9321, -3.0183],
         [-2.8522,  6.4450, -3.9181,  ...,  2.0990, -1.6254, -2.5238]],

        [[-2.6405,  6.0978, -3.5529,  ...,  1.5928, -0.7095, -1.4298],
         [ 0.4645,  6.0594, -1.6437,  ...,  1.2673, -0.5592, -4.0654],
         [ 0.2677,  6.5881, -1.9177,  ...,  1.9501, -1.4949, -4.7650]],

        [[ 2.7513,  6.2128, -0.9586,  ..., -0.0430,  0.6775, -1.8767],
         [ 2.9061,  5.5429, -0.4296,  ...,  0.5124, -0.3556, -3.6117],
         [ 0.1698,  7.0650, -1.9781,  ...,  2.1477, -1.3450, -5.1237]],

        [[ 0.4863,  6.4957, -0.7849,  ...,  1.2298, -0.4106, -3.1129],
         [-0.6274,  6.7751, -1.4143,  ...,  2.3529, -0.6790, -2.9196],
         [-1.7043,  4.7040, -3.6641,  ...,  1.5650, -1.9659, -3.0286]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:mmr!
INFO:root:query size before linear: torch.Size([32, 3, 128])
INFO:root:doc emb torch.Size([5, 3, 128])
INFO:root:query after linear torch.Size([3, 32, 128]) 
INFO:root:query attn after linear: torch.Size([3, 32, 1])
INFO:root:doc emb torch.Size([3, 5, 128])
INFO:root:query doc linear torch.Size([3, 32, 5])
INFO:root:tensor([[[ 12.8422,  11.7705,  18.3798,  12.5090,  13.1066],
         [ 13.8395,  15.7293,  14.9774,  17.6014,  16.5587],
         [ 18.1470,  20.0161,  17.2174,  20.4573,  22.1275],
         [ -6.5261,  -5.0691,   4.2160,  -4.1349,  -0.4701],
         [  7.0985,   9.3512,   7.3402,  12.1610,   9.0233],
         [-10.5452,  -9.9937,   2.9044,  -8.6487,  -7.2398],
         [ 10.6028,   9.0969,  10.9472,  10.2746,  10.3141],
         [  8.1626,   7.7473,   7.6298,  10.2217,  11.6306],
         [  1.8763,  -0.2941,   5.4883,   0.9196,   0.2508],
         [ 11.7856,   9.9599,  11.9221,   9.9351,  10.5835],
         [ 12.2906,  15.4199,  13.8112,  15.0518,  14.7842],
         [  8.4068,   6.7879,   7.4158,   6.8513,   6.8391],
         [ 28.8593,  28.2092,  23.0854,  28.4469,  26.0021],
         [ 40.2555,  38.0988,  33.7101,  38.2792,  33.0172],
         [ 12.5693,  12.1528,   8.3427,  14.4121,   8.6736],
         [  6.7215,   4.8694,  14.4658,   7.1440,   2.1131],
         [ 35.6138,  34.2809,  25.6922,  34.0506,  28.8787],
         [ 24.4618,  21.3286,  20.0930,  23.0059,  21.0914],
         [ 21.8419,  19.5862,  24.2836,  22.7704,  20.2988],
         [ 23.7675,  20.0146,  16.7356,  22.9211,  18.8814],
         [ 15.6297,  16.1291,  16.6212,  16.0374,  12.9742],
         [ 25.5950,  22.9080,  23.2648,  23.0477,  23.8910],
         [ 18.5892,  16.2501,  17.4780,  19.0893,  14.9700],
         [ 15.8012,  12.9540,  21.3568,  14.6268,  12.3308],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf]],

        [[  3.8506,   1.6284,  10.4735,   9.8852,  13.3130],
         [ 13.3274,  12.3364,  24.3937,  26.1825,  28.0296],
         [  7.9653,   6.7513,   6.6892,   3.6634,   9.4685],
         [  5.2492,   3.5242,   8.0308,  10.3536,   6.9207],
         [  8.6441,   8.4260,  16.1565,  19.2346,  16.9689],
         [-13.2734, -14.6849,  -8.7921,  -4.5886,  -7.4281],
         [ -7.2419,  -8.9128,  -5.5436,  -0.3369,  -2.0607],
         [ 10.2744,   8.3487,  16.6781,  19.2845,  16.1740],
         [  8.4395,   6.2626,  17.6405,  17.1846,  16.4637],
         [ 10.8832,   9.2265,  13.4908,  16.8836,  10.2878],
         [ 15.8061,  14.3555,  26.2623,  29.3075,  23.4735],
         [ 10.0720,   8.1781,  14.2053,  16.4782,  11.4665],
         [ 17.6309,  18.0021,  22.9707,  24.6447,  26.2455],
         [ 26.9976,  24.8662,  38.3508,  37.8487,  37.9791],
         [ 13.3853,  14.2590,  17.8686,  19.0982,  22.2437],
         [  0.4866,  -0.1605,   5.7318,   9.9871,  10.7583],
         [ -2.9979,  -3.9344,   2.5924,   5.9144,   4.5743],
         [ 13.5433,  13.6853,  25.8297,  27.6137,  26.9628],
         [  6.3571,   6.5117,  16.5471,  17.9082,  18.6497],
         [ 22.4701,  22.0949,  29.4609,  28.5914,  30.1740],
         [ 22.2184,  23.6310,  29.9653,  24.6171,  34.2801],
         [ 22.2307,  21.6608,  29.3863,  28.9455,  34.0630],
         [ 19.1030,  18.4795,  22.3554,  20.6602,  19.1065],
         [ 13.3840,  13.5987,  20.7097,  18.1714,  21.8821],
         [  2.0248,   2.6885,   7.4446,  12.0218,   5.1270],
         [ 14.1249,  13.9136,  14.2996,  16.7786,  18.2563],
         [ 12.8238,  13.2898,  13.4409,  15.6572,  15.0109],
         [ 11.1336,  13.2434,   1.7504,   3.5252,   2.3746],
         [  8.2599,   8.0359,   7.7283,   7.3655,   8.6040],
         [ 29.4527,  28.7275,  30.3624,  26.4531,  34.4004],
         [ 24.2928,  26.2092,  20.7946,  19.6933,  23.7446],
         [ 10.5274,  12.0103,  12.3689,  10.0704,  16.6653]],

        [[  8.9828,  18.3867,  11.1701,  11.2943,  13.3072],
         [ 33.1641,  30.7643,  36.1177,  37.1002,  23.5324],
         [ 11.4150,  15.3997,  13.9768,  15.1402,  12.2281],
         [  4.2104,  10.6705,   5.8434,   6.4752,   7.2478],
         [ 16.7730,  16.3626,  17.7320,  19.0370,  12.0117],
         [ -6.0845,   4.1355,  -4.3633,  -3.3557,  -1.4798],
         [  5.2790,  10.4519,   6.4789,   4.6707,   9.2419],
         [ 18.3271,  15.0342,  17.9059,  17.7401,  11.9513],
         [ -6.9650,   0.5442,  -6.1221,  -6.3432,  -3.9712],
         [ 10.1505,   8.0106,   9.4981,   9.4687,   9.6787],
         [ 17.1502,  11.8881,  17.2504,  17.1910,   8.5523],
         [ 11.0245,  10.2828,  11.9636,  10.8367,   7.8926],
         [ 18.1101,  15.9599,  20.1460,  19.9730,  12.2076],
         [ 27.3814,  31.0631,  30.7211,  30.9099,  22.8069],
         [ 11.4190,  11.6856,  12.8563,  13.9248,   9.0345],
         [ 11.0194,  20.5340,  12.2094,  13.8582,  15.9252],
         [ 14.3977,  19.2523,  16.3336,  16.7511,  15.1077],
         [ 16.6729,  14.3132,  16.8532,  18.9863,   9.3624],
         [ 22.4714,  19.8416,  22.9357,  25.3913,  14.6959],
         [ 22.7579,  14.8879,  22.4394,  24.2773,  16.7037],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf],
         [    -inf,     -inf,     -inf,     -inf,     -inf]]], device='cuda:0',
       grad_fn=<MaskedFillBackward0>)
INFO:root:tensor([[[ 0.3714,  0.3404,  0.5316,  0.3618,  0.3791],
         [ 0.3203,  0.3640,  0.3466,  0.4074,  0.3832],
         [ 0.3745,  0.4131,  0.3553,  0.4222,  0.4566],
         [-0.1880, -0.1460,  0.1215, -0.1191, -0.0135],
         [ 0.2303,  0.3033,  0.2381,  0.3945,  0.2927],
         [-0.3525, -0.3341,  0.0971, -0.2891, -0.2420],
         [ 0.4441,  0.3810,  0.4585,  0.4304,  0.4320],
         [ 0.3388,  0.3216,  0.3167,  0.4243,  0.4828],
         [ 0.0746, -0.0117,  0.2183,  0.0366,  0.0100],
         [ 0.4835,  0.4086,  0.4891,  0.4076,  0.4342],
         [ 0.3921,  0.4919,  0.4406,  0.4802,  0.4717],
         [ 0.1959,  0.1582,  0.1729,  0.1597,  0.1594],
         [ 0.6653,  0.6503,  0.5322,  0.6558,  0.5995],
         [ 0.9158,  0.8667,  0.7669,  0.8708,  0.7511],
         [ 0.3345,  0.3234,  0.2220,  0.3835,  0.2308],
         [ 0.1948,  0.1411,  0.4193,  0.2070,  0.0612],
         [ 0.9485,  0.9130,  0.6843,  0.9069,  0.7691],
         [ 0.7215,  0.6291,  0.5926,  0.6786,  0.6221],
         [ 0.6604,  0.5922,  0.7342,  0.6885,  0.6138],
         [ 0.5495,  0.4627,  0.3869,  0.5299,  0.4365],
         [ 0.3946,  0.4072,  0.4196,  0.4049,  0.3275],
         [ 0.9158,  0.8196,  0.8324,  0.8246,  0.8548],
         [ 0.5758,  0.5033,  0.5414,  0.5913,  0.4637],
         [ 0.3167,  0.2596,  0.4280,  0.2931,  0.2471],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf]],

        [[ 0.1347,  0.0569,  0.3663,  0.3457,  0.4656],
         [ 0.3494,  0.3235,  0.6396,  0.6865,  0.7349],
         [ 0.1781,  0.1509,  0.1496,  0.0819,  0.2117],
         [ 0.1475,  0.0990,  0.2256,  0.2908,  0.1944],
         [ 0.2558,  0.2494,  0.4782,  0.5693,  0.5022],
         [-0.4545, -0.5028, -0.3010, -0.1571, -0.2543],
         [-0.2937, -0.3614, -0.2248, -0.0137, -0.0836],
         [ 0.4791,  0.3893,  0.7777,  0.8992,  0.7542],
         [ 0.2913,  0.2162,  0.6090,  0.5932,  0.5683],
         [ 0.5917,  0.5016,  0.7334,  0.9179,  0.5593],
         [ 0.4534,  0.4118,  0.7533,  0.8406,  0.6733],
         [ 0.2111,  0.1714,  0.2977,  0.3453,  0.2403],
         [ 0.4005,  0.4089,  0.5218,  0.5598,  0.5962],
         [ 0.5982,  0.5509,  0.8497,  0.8386,  0.8415],
         [ 0.3983,  0.4243,  0.5317,  0.5682,  0.6618],
         [ 0.0210, -0.0069,  0.2469,  0.4302,  0.4634],
         [-0.0947, -0.1242,  0.0819,  0.1868,  0.1445],
         [ 0.3241,  0.3275,  0.6181,  0.6607,  0.6452],
         [ 0.1894,  0.1940,  0.4930,  0.5335,  0.5556],
         [ 0.6035,  0.5934,  0.7913,  0.7679,  0.8104],
         [ 0.5436,  0.5782,  0.7332,  0.6023,  0.8387],
         [ 0.7264,  0.7078,  0.9602,  0.9458,  1.1130],
         [ 0.6710,  0.6491,  0.7853,  0.7257,  0.6711],
         [ 0.4529,  0.4602,  0.7008,  0.6149,  0.7405],
         [ 0.0484,  0.0642,  0.1778,  0.2872,  0.1225],
         [ 0.5019,  0.4944,  0.5082,  0.5963,  0.6488],
         [ 0.4514,  0.4678,  0.4732,  0.5512,  0.5284],
         [ 0.2854,  0.3395,  0.0449,  0.0904,  0.0609],
         [ 0.3111,  0.3027,  0.2911,  0.2774,  0.3241],
         [ 0.9067,  0.8844,  0.9347,  0.8143,  1.0590],
         [ 0.6423,  0.6930,  0.5498,  0.5207,  0.6279],
         [ 0.2917,  0.3328,  0.3427,  0.2791,  0.4618]],

        [[ 0.2614,  0.5351,  0.3251,  0.3287,  0.3873],
         [ 0.7146,  0.6629,  0.7782,  0.7994,  0.5071],
         [ 0.2669,  0.3600,  0.3267,  0.3539,  0.2859],
         [ 0.1287,  0.3263,  0.1787,  0.1980,  0.2216],
         [ 0.4417,  0.4308,  0.4669,  0.5013,  0.3163],
         [-0.2177,  0.1480, -0.1561, -0.1201, -0.0530],
         [ 0.1673,  0.3313,  0.2053,  0.1480,  0.2929],
         [ 0.8384,  0.6877,  0.8191,  0.8115,  0.5467],
         [-0.2561,  0.0200, -0.2251, -0.2333, -0.1460],
         [ 0.4052,  0.3198,  0.3792,  0.3780,  0.3864],
         [ 0.4320,  0.2995,  0.4345,  0.4330,  0.2154],
         [ 0.2220,  0.2071,  0.2409,  0.2182,  0.1590],
         [ 0.4552,  0.4012,  0.5064,  0.5020,  0.3069],
         [ 0.6143,  0.6969,  0.6892,  0.6934,  0.5117],
         [ 0.3026,  0.3097,  0.3407,  0.3690,  0.2394],
         [ 0.2196,  0.4093,  0.2434,  0.2762,  0.3174],
         [ 0.3748,  0.5011,  0.4251,  0.4360,  0.3932],
         [ 0.3204,  0.2750,  0.3239,  0.3648,  0.1799],
         [ 0.6493,  0.5733,  0.6627,  0.7337,  0.4246],
         [ 0.5897,  0.3858,  0.5815,  0.6291,  0.4328],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf],
         [   -inf,    -inf,    -inf,    -inf,    -inf]]], device='cuda:0',
       grad_fn=<MulBackward0>)
INFO:root:weighted query doc linear torch.Size([3, 32, 5])
INFO:root:tensor([[   -inf,    -inf,    -inf,    -inf,    -inf],
        [10.6169, 10.0476, 15.1403, 16.2506, 16.4814],
        [   -inf,    -inf,    -inf,    -inf,    -inf]], device='cuda:0',
       grad_fn=<SumBackward1>)
INFO:root:first sim: torch.Size([3, 5])
INFO:root:doc weights torch.Size([3, 5, 128])
INFO:root:sim 2 torch.Size([3, 5, 5])
INFO:root:sim 2 tensor([[[568.0330, 553.0957, 380.3165, 535.2738, 520.3430],
         [553.0957, 562.1033, 385.1621, 540.5388, 524.6415],
         [380.3165, 385.1621, 352.9373, 362.6789, 358.7269],
         [535.2738, 540.5388, 362.6789, 529.8938, 505.2024],
         [520.3430, 524.6415, 358.7269, 505.2024, 511.1871]],

        [[452.3056, 452.4927, 483.1153, 417.7133, 485.3974],
         [452.4927, 457.7559, 480.5883, 414.2851, 486.7776],
         [483.1153, 480.5883, 593.8418, 532.8744, 597.4443],
         [417.7133, 414.2851, 532.8744, 503.3218, 533.0273],
         [485.3974, 486.7776, 597.4443, 533.0273, 617.8263]],

        [[572.0903, 392.2697, 569.1031, 581.9264, 360.9712],
         [392.2697, 346.2234, 389.9938, 398.8546, 299.2787],
         [569.1031, 389.9938, 571.2142, 582.4529, 360.0807],
         [581.9264, 398.8546, 582.4529, 596.8901, 366.4103],
         [360.9712, 299.2787, 360.0807, 366.4103, 280.8737]]], device='cuda:0',
       grad_fn=<BmmBackward>)
INFO:root:mmr_scores torch.Size([3, 5, 5])
INFO:root:mmr_scores tensor([[[     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf]],

        [[-220.8443, -220.9379, -236.2492, -203.5482, -237.3902],
         [-221.2225, -223.8541, -235.2704, -202.1187, -238.3650],
         [-233.9875, -232.7240, -289.3507, -258.8670, -291.1520],
         [-200.7314, -199.0172, -258.3119, -243.5356, -258.3883],
         [-234.4580, -235.1481, -290.4815, -258.2729, -300.6725]],

        [[     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf]]],
       device='cuda:0', grad_fn=<SubBackward0>)
INFO:root:mmr_scores torch.Size([3, 5])
INFO:root:tensor([[[ 1.7081,  6.6247, -0.9537,  ...,  0.5410, -0.5112, -2.0923],
         [ 1.4835,  6.1515, -1.4281,  ...,  0.7242, -0.0603, -1.7523],
         [-2.6405,  6.0978, -3.5529,  ...,  1.5928, -0.7095, -1.4298],
         [ 2.7513,  6.2128, -0.9586,  ..., -0.0430,  0.6775, -1.8767],
         [ 0.4863,  6.4957, -0.7849,  ...,  1.2298, -0.4106, -3.1129]],

        [[ 0.8990,  4.0860, -1.8945,  ...,  1.5545, -1.1429, -3.2370],
         [ 1.0401,  4.3158, -1.4345,  ...,  1.5483, -0.9321, -3.0183],
         [ 0.4645,  6.0594, -1.6437,  ...,  1.2673, -0.5592, -4.0654],
         [ 2.9061,  5.5429, -0.4296,  ...,  0.5124, -0.3556, -3.6117],
         [-0.6274,  6.7751, -1.4143,  ...,  2.3529, -0.6790, -2.9196]],

        [[ 0.0820,  6.7393, -1.9505,  ...,  2.0692, -1.9531, -4.8317],
         [-2.8522,  6.4450, -3.9181,  ...,  2.0990, -1.6254, -2.5238],
         [ 0.2677,  6.5881, -1.9177,  ...,  1.9501, -1.4949, -4.7650],
         [ 0.1698,  7.0650, -1.9781,  ...,  2.1477, -1.3450, -5.1237],
         [-1.7043,  4.7040, -3.6641,  ...,  1.5650, -1.9659, -3.0286]]],
       device='cuda:0', grad_fn=<TransposeBackward0>)
INFO:root:tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],

        [[ 0.8990,  4.0860, -1.8945,  ...,  1.5545, -1.1429, -3.2370],
         [ 1.0401,  4.3158, -1.4345,  ...,  1.5483, -0.9321, -3.0183],
         [ 0.3621,  4.7240, -1.2814,  ...,  0.9880, -0.4360, -3.1695],
         [ 2.4625,  4.6969, -0.3640,  ...,  0.4342, -0.3013, -3.0605],
         [-0.4178,  4.5122, -0.9419,  ...,  1.5670, -0.4522, -1.9445]],

        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc_emb torch.Size([3, 5, 128])
INFO:root:doc mmr weighted size torch.Size([5, 3, 128])
INFO:root:input for adding context torch.Size([32, 5, 3, 128])
DEBUG:root:local doc emb size after global enc torch.Size([32, 15, 128]):
DEBUG:root:global doc emb size after global_enc torch.Size([5, 3, 128]):
DEBUG:root:query emb size after global_enc torch.Size([32, 3, 128]):
INFO:root:Using global transformer
INFO:root:initial input tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:input_size torch.Size([32, 15, 128])
INFO:root:query size torch.Size([32, 3, 128])
INFO:root:src padding_mask size torch.Size([15, 32])
INFO:root:Computing multi-head pooling
INFO:root:tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:attn scores torch.Size([32, 15, 4])
INFO:root:values torch.Size([32, 15, 128])
WARNING:root:Fully masked documents will result in nan
INFO:root:scores torch.Size([32, 60, 1])
INFO:root:values after shaping torch.Size([32, 60, 32])
INFO:root:tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:tokens after weightin torch.Size([32, 60, 32])
INFO:root:tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         ...,
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       grad_fn=<MulBackward0>)
INFO:root:doc representation after sumtorch.Size([60, 32])
INFO:root:tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
       grad_fn=<SumBackward1>)
INFO:root:doc representation reshpae torch.Size([5, 3, 128])
INFO:root:doc representation torch.Size([5, 3, 128])
INFO:root:tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  7.4428, -14.2604,  13.6852,  ...,   3.4114,  -7.1733,  11.1822],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 14.5700, -10.4170,  15.6877,  ...,  -0.7074,  -7.6924,  11.4175],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 15.2702, -16.1365,  16.8765,  ...,   3.1936,  -4.8392,   9.1832],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 11.1630,  -9.0849,  12.4737,  ...,   1.9819,  -5.4502,   4.7536],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 12.5608, -13.7660,  17.4769,  ...,   0.7307,  -9.6035,  13.6107],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:Pooling size torch.Size([5, 3, 128])
INFO:root:x pooled tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  7.4428, -14.2604,  13.6852,  ...,   3.4114,  -7.1733,  11.1822],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 14.5700, -10.4170,  15.6877,  ...,  -0.7074,  -7.6924,  11.4175],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 15.2702, -16.1365,  16.8765,  ...,   3.1936,  -4.8392,   9.1832],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 11.1630,  -9.0849,  12.4737,  ...,   1.9819,  -5.4502,   4.7536],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [ 12.5608, -13.7660,  17.4769,  ...,   0.7307,  -9.6035,  13.6107],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:doc attn size torch.Size([5, 3, 128])
INFO:root:inter doc attn tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  4.3317,  -0.2304,  -4.7345,  ..., -12.7304,  -6.1909,  -4.3229],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  4.1185,  -0.1360,  -4.8130,  ..., -12.1010,  -6.0464,  -4.0692],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  0.1265,   0.0130,  -2.9231,  ..., -11.2679,  -0.9869,  -3.1298],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  4.3127,  -0.2887,  -4.7571,  ..., -12.7433,  -6.2138,  -4.3721],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [  4.8212,  -0.7657,  -2.4178,  ..., -11.0378,  -5.7879,  -3.7784],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],
       device='cuda:0', grad_fn=<AddBackward0>)
INFO:root:mmr!
INFO:root:query size before linear: torch.Size([32, 3, 128])
INFO:root:doc emb torch.Size([5, 3, 128])
INFO:root:query after linear torch.Size([3, 32, 128]) 
INFO:root:query attn after linear: torch.Size([3, 32, 1])
INFO:root:doc emb torch.Size([3, 5, 128])
INFO:root:query doc linear torch.Size([3, 32, 5])
INFO:root:tensor([[[        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf]],

        [[-7.9796e+00, -6.9130e+00, -6.3918e+00, -7.8849e+00, -6.3165e+00],
         [ 9.2954e+00,  9.0632e+00,  8.7128e+00,  9.0937e+00,  1.1573e+01],
         [ 6.1127e+00,  6.1459e+00,  1.0116e+01,  6.3729e+00,  1.6093e+00],
         [-5.0272e-01,  5.2823e-03,  9.9180e+00, -6.0587e-01,  1.6905e+00],
         [-1.5238e+01, -1.4977e+01, -7.1061e+00, -1.5256e+01, -4.2032e-01],
         [ 1.7878e+00,  1.9147e+00, -7.3217e-01,  1.7795e+00,  1.1471e+01],
         [-2.1032e+01, -2.0350e+01, -8.3959e+00, -2.1000e+01, -1.8340e+01],
         [-2.6535e+01, -2.5385e+01, -1.2094e+01, -2.6497e+01, -2.3729e+01],
         [-8.5358e+00, -8.2841e+00,  8.3546e-01, -8.6942e+00,  4.6707e-01],
         [-2.7723e+01, -2.7527e+01, -1.4118e+01, -2.7568e+01, -1.3862e+01],
         [-1.8310e+01, -1.7682e+01, -6.3823e+00, -1.8418e+01, -1.6687e+01],
         [-1.0545e+01, -9.2458e+00, -3.5165e-01, -1.0810e+01, -1.0313e+01],
         [-6.7200e+00, -6.9556e+00,  5.0226e+00, -6.6827e+00, -2.3036e+00],
         [-1.4081e+01, -1.3373e+01, -8.1719e+00, -1.4286e+01, -7.7248e+00],
         [-2.3915e+01, -2.2952e+01, -2.1496e+01, -2.4082e+01, -7.3243e+00],
         [-2.1889e+01, -2.1519e+01, -2.4350e+01, -2.2149e+01, -5.6072e+00],
         [ 1.8728e+00,  2.3084e+00,  1.1018e+01,  1.7922e+00,  1.9419e+00],
         [ 5.0120e+00,  4.3122e+00,  1.5190e+01,  4.7493e+00,  1.2444e+01],
         [ 2.4602e+01,  2.4532e+01,  2.6400e+01,  2.4184e+01,  2.0238e+01],
         [-2.6842e+01, -2.5279e+01, -2.1959e+01, -2.6939e+01, -2.0553e+01],
         [ 1.6095e+01,  1.6465e+01,  2.0840e+00,  1.5933e+01,  1.6817e+01],
         [ 1.4833e+01,  1.4818e+01,  1.4049e+01,  1.4765e+01,  1.3935e+01],
         [-8.9077e+00, -8.6023e+00, -4.7147e+00, -9.0123e+00, -1.3703e+00],
         [-4.3580e+00, -4.6251e+00, -3.6857e+00, -4.1348e+00,  3.2251e+00],
         [-7.6782e+00, -8.8483e+00,  3.8179e+00, -7.7634e+00,  1.6985e+00],
         [-2.3316e+01, -2.3565e+01, -2.1010e+01, -2.3375e+01, -7.7040e+00],
         [-1.9930e+01, -1.9410e+01, -1.4755e+01, -2.0110e+01, -1.1849e+01],
         [-3.4050e+01, -3.2570e+01, -3.7253e+01, -3.3997e+01, -1.9039e+01],
         [-2.1538e+01, -2.0078e+01, -1.9963e+01, -2.1441e+01, -2.1211e+01],
         [-1.9519e+01, -1.7743e+01, -2.6303e+01, -1.9549e+01, -1.9755e+01],
         [-7.9985e+00, -7.0881e+00, -1.1993e+01, -7.8824e+00, -9.4456e+00],
         [-5.7131e+00, -4.8258e+00, -7.3055e+00, -6.0376e+00,  7.8790e-01]],

        [[        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf]]],
       device='cuda:0', grad_fn=<MaskedFillBackward0>)
INFO:root:tensor([[[        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf]],

        [[-2.7904e-01, -2.4174e-01, -2.2352e-01, -2.7573e-01, -2.2089e-01],
         [ 2.4372e-01,  2.3764e-01,  2.2845e-01,  2.3844e-01,  3.0343e-01],
         [ 1.3666e-01,  1.3740e-01,  2.2618e-01,  1.4248e-01,  3.5979e-02],
         [-1.4122e-02,  1.4838e-04,  2.7860e-01, -1.7019e-02,  4.7487e-02],
         [-4.5103e-01, -4.4328e-01, -2.1033e-01, -4.5155e-01, -1.2441e-02],
         [ 6.1214e-02,  6.5560e-02, -2.5069e-02,  6.0929e-02,  3.9277e-01],
         [-8.5288e-01, -8.2524e-01, -3.4047e-01, -8.5160e-01, -7.4375e-01],
         [-1.2373e+00, -1.1837e+00, -5.6397e-01, -1.2356e+00, -1.1065e+00],
         [-2.9466e-01, -2.8597e-01,  2.8840e-02, -3.0012e-01,  1.6123e-02],
         [-1.5072e+00, -1.4965e+00, -7.6752e-01, -1.4987e+00, -7.5364e-01],
         [-5.2517e-01, -5.0716e-01, -1.8306e-01, -5.2827e-01, -4.7863e-01],
         [-2.2096e-01, -1.9374e-01, -7.3685e-03, -2.2652e-01, -2.1610e-01],
         [-1.5264e-01, -1.5800e-01,  1.1409e-01, -1.5180e-01, -5.2325e-02],
         [-3.1197e-01, -2.9629e-01, -1.8106e-01, -3.1651e-01, -1.7115e-01],
         [-7.1156e-01, -6.8291e-01, -6.3959e-01, -7.1653e-01, -2.1792e-01],
         [-9.4280e-01, -9.2685e-01, -1.0488e+00, -9.5399e-01, -2.4151e-01],
         [ 5.9142e-02,  7.2896e-02,  3.4793e-01,  5.6595e-02,  6.1325e-02],
         [ 1.1993e-01,  1.0318e-01,  3.6347e-01,  1.1364e-01,  2.9776e-01],
         [ 7.3292e-01,  7.3083e-01,  7.8650e-01,  7.2047e-01,  6.0291e-01],
         [-7.2092e-01, -6.7894e-01, -5.8977e-01, -7.2354e-01, -5.5201e-01],
         [ 3.9380e-01,  4.0287e-01,  5.0990e-02,  3.8984e-01,  4.1146e-01],
         [ 4.8467e-01,  4.8418e-01,  4.5907e-01,  4.8247e-01,  4.5534e-01],
         [-3.1290e-01, -3.0217e-01, -1.6561e-01, -3.1657e-01, -4.8132e-02],
         [-1.4747e-01, -1.5651e-01, -1.2472e-01, -1.3992e-01,  1.0914e-01],
         [-1.8340e-01, -2.1135e-01,  9.1195e-02, -1.8544e-01,  4.0571e-02],
         [-8.2857e-01, -8.3741e-01, -7.4662e-01, -8.3066e-01, -2.7377e-01],
         [-7.0161e-01, -6.8330e-01, -5.1943e-01, -7.0794e-01, -4.1711e-01],
         [-8.7279e-01, -8.3486e-01, -9.5487e-01, -8.7142e-01, -4.8801e-01],
         [-8.1123e-01, -7.5624e-01, -7.5191e-01, -8.0758e-01, -7.9891e-01],
         [-6.0086e-01, -5.4620e-01, -8.0971e-01, -6.0179e-01, -6.0814e-01],
         [-2.1149e-01, -1.8742e-01, -3.1713e-01, -2.0843e-01, -2.4976e-01],
         [-1.5831e-01, -1.3372e-01, -2.0244e-01, -1.6730e-01,  2.1833e-02]],

        [[        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,         nan],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,        -inf]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:weighted query doc linear torch.Size([3, 32, 5])
INFO:root:tensor([[     nan,      nan,      nan,      nan,      nan],
        [-10.8188, -10.3349,  -6.3977, -10.8797,  -4.8546],
        [     nan,      nan,      nan,      nan,      nan]], device='cuda:0',
       grad_fn=<SumBackward1>)
INFO:root:first sim: torch.Size([3, 5])
INFO:root:doc weights torch.Size([3, 5, 128])
INFO:root:sim 2 torch.Size([3, 5, 5])
INFO:root:sim 2 tensor([[[      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan]],

        [[1307.3932, 1273.0947,  982.7856, 1306.7925, 1013.2547],
         [1273.0947, 1242.0751,  946.0965, 1272.3842,  980.0834],
         [ 982.7856,  946.0965,  977.5350,  982.9645,  726.8300],
         [1306.7925, 1272.3842,  982.9645, 1306.2922, 1012.2238],
         [1013.2547,  980.0834,  726.8300, 1012.2238,  916.7471]],

        [[      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan]]],
       device='cuda:0', grad_fn=<BmmBackward>)
INFO:root:mmr_scores torch.Size([3, 5, 5])
INFO:root:mmr_scores tensor([[[      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan]],

        [[-659.1060, -641.9568, -496.8022, -658.8057, -512.0367],
         [-641.7148, -626.2050, -478.2157, -641.3595, -495.2091],
         [-494.5916, -476.2471, -491.9663, -494.6811, -366.6138],
         [-658.8361, -641.6319, -496.9221, -658.5859, -511.5518],
         [-509.0546, -492.4690, -365.8423, -508.5392, -460.8008]],

        [[      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan],
         [      nan,       nan,       nan,       nan,       nan]]],
       device='cuda:0', grad_fn=<SubBackward0>)
INFO:root:mmr_scores torch.Size([3, 5])
INFO:root:tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[  4.3317,  -0.2304,  -4.7345,  ..., -12.7304,  -6.1909,  -4.3229],
         [  4.1185,  -0.1360,  -4.8130,  ..., -12.1010,  -6.0464,  -4.0692],
         [  0.1265,   0.0130,  -2.9231,  ..., -11.2679,  -0.9869,  -3.1298],
         [  4.3127,  -0.2887,  -4.7571,  ..., -12.7433,  -6.2138,  -4.3721],
         [  4.8212,  -0.7657,  -2.4178,  ..., -11.0378,  -5.7879,  -3.7784]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],
       device='cuda:0', grad_fn=<TransposeBackward0>)
INFO:root:tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],

        [[  4.3317,  -0.2304,  -4.7345,  ..., -12.7304,  -6.1909,  -4.3229],
         [  4.1185,  -0.1360,  -4.8130,  ..., -12.1010,  -6.0464,  -4.0692],
         [  0.1265,   0.0130,  -2.9231,  ..., -11.2679,  -0.9869,  -3.1298],
         [  4.3127,  -0.2887,  -4.7571,  ..., -12.7433,  -6.2138,  -4.3721],
         [  4.8212,  -0.7657,  -2.4178,  ..., -11.0378,  -5.7879,  -3.7784]],

        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],
         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],
       device='cuda:0', grad_fn=<MulBackward0>)
INFO:root:doc_emb torch.Size([3, 5, 128])
INFO:root:doc mmr weighted size torch.Size([5, 3, 128])
INFO:root:input for adding context torch.Size([32, 5, 3, 128])
DEBUG:root:local doc emb size after global enc torch.Size([32, 15, 128]):
DEBUG:root:global doc emb size after global_enc torch.Size([5, 3, 128]):
DEBUG:root:query emb size after global_enc torch.Size([32, 3, 128]):
DEBUG:root:global doc emb size after encoder torch.Size([5, 3, 128]):
DEBUG:root:local doc emb size after encoder torch.Size([32, 15, 128]):
INFO:root:tgt attn mask torch.Size([32, 32])
WARNING:root:Use global or local representations for decoder?
DEBUG:root:output after linear layer: torch.Size([32, 3, 50265])
DEBUG:root:output torch.Size([3, 50265, 32])
INFO:root:tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         ...,
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],

        [[-0.4958, -0.3222, -0.9173,  ..., -0.2644, -0.2179, -0.2190],
         [ 0.6022,  0.7941,  0.8175,  ...,  1.0746,  0.2312,  1.7116],
         [-0.3082, -0.5269, -0.2398,  ..., -0.1235, -0.0563, -0.2265],
         ...,
         [-0.6191, -0.2588, -0.3019,  ..., -0.4476, -0.1969, -0.3323],
         [-0.1253, -0.2959, -0.5204,  ..., -0.5150,  0.1539, -0.0471],
         [-0.6922, -0.7698, -0.8272,  ..., -0.8826, -0.7617, -1.0792]],

        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         ...,
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
       device='cuda:0', grad_fn=<PermuteBackward>)
INFO:root:Batch # 0
WARNING:root:FORCED EXIT!
