INFO:root:Encoder objects: ModuleList(
  (0): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerGlobalEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=6, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=6, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerGlobalEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=6, out_features=6, bias=True)
        )
        (linear1): Linear(in_features=6, out_features=6, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=6, out_features=6, bias=True)
        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
INFO:root:Initializing random embeddings
INFO:root:Using cuda:0
INFO:root:Beginning training for 2 epochs, batch size 2
DEBUG:root:dob emb size for input: torch.Size([3, 2, 4, 6])
WARNING:root:Implement use of query emb!
INFO:root:seq len: 4, batch: 2, docs: 3, h: 6
INFO:root:torch.Size([4, 6, 6])
INFO:root:torch.Size([4, 2, 6])
DEBUG:root:dob emb size: torch.Size([4, 6, 6])
DEBUG:root:dob emb size after pos: torch.Size([4, 6, 6])
INFO:root:Contiguous: True
INFO:root:passing input through enc
INFO:root:cuda:0
DEBUG:root:layer type: local
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([4, 6, 6]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
DEBUG:root:local doc emb size after global enc torch.Size([4, 6, 6]):
DEBUG:root:global doc emb size after global_enc torch.Size([3, 2, 6]):
DEBUG:root:global doc emb size after encoder torch.Size([3, 2, 6]):
DEBUG:root:local doc emb size after encoder torch.Size([4, 6, 6]):
INFO:root:Topk!
INFO:root:tensor([[[-1.5900, -1.9456, -1.9414, -1.6892, -1.8565, -1.5223],
         [-1.8794, -1.5616, -1.6609, -1.8896, -1.6922, -2.0177]],

        [[-1.7418, -2.0134, -1.9558, -1.7178, -1.8965, -1.5607],
         [-1.9558, -1.5835, -1.6689, -1.9029, -1.6982, -2.0432]]],
       device='cuda:0', grad_fn=<TopkBackward>)
INFO:root:torch.Size([2, 2, 6])
INFO:root:tensor([[[ 0.5684, -0.1987, -0.1340,  0.2124],
         [ 0.2031,  0.0994,  0.0741, -0.6308],
         [-0.2480,  0.3309,  0.4164, -0.3219],
         ...,
         [ 0.4948, -0.3609,  0.0775, -0.5400],
         [ 0.2369,  0.2027, -0.0807,  0.0994],
         [ 0.4237, -0.0805,  0.0167, -0.2072]],

        [[ 0.3171, -0.6230, -0.2664, -0.2646],
         [ 0.0466, -0.2410, -0.1320, -0.0108],
         [-0.6844,  0.4444,  0.1024, -0.6093],
         ...,
         [ 0.2842, -0.1792,  0.0502, -0.5084],
         [ 0.2024,  0.1479, -0.1176,  0.0070],
         [ 0.3611, -0.2354, -0.0987, -0.0270]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:tensor([[1, 1, 7, 9],
        [0, 1, 3, 0]], device='cuda:0')
DEBUG:root:dob emb size for input: torch.Size([3, 2, 4, 6])
WARNING:root:Implement use of query emb!
INFO:root:seq len: 4, batch: 2, docs: 3, h: 6
INFO:root:torch.Size([4, 6, 6])
INFO:root:torch.Size([4, 2, 6])
DEBUG:root:dob emb size: torch.Size([4, 6, 6])
DEBUG:root:dob emb size after pos: torch.Size([4, 6, 6])
INFO:root:Contiguous: True
INFO:root:passing input through enc
INFO:root:cuda:0
DEBUG:root:layer type: local
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([4, 6, 6]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
DEBUG:root:local doc emb size after global enc torch.Size([4, 6, 6]):
DEBUG:root:global doc emb size after global_enc torch.Size([3, 2, 6]):
DEBUG:root:global doc emb size after encoder torch.Size([3, 2, 6]):
DEBUG:root:local doc emb size after encoder torch.Size([4, 6, 6]):
INFO:root:Topk!
INFO:root:tensor([[[   0.0000, -284.6023, -342.7817, -449.2397, -236.8331, -324.6748],
         [   0.0000,  -31.7073,  -32.1866,    0.0000,  -17.2872,  -35.6219]],

        [[   0.0000, -284.6023, -342.7817, -449.2397, -236.8331, -324.6748],
         [   0.0000, -133.3700, -160.3431, -203.9330, -102.5891, -152.1733]]],
       device='cuda:0', grad_fn=<TopkBackward>)
INFO:root:torch.Size([2, 2, 6])
INFO:root:tensor([[[-18.4667,  -6.3442,  -3.3781, -29.2068],
         [ 25.5389,   4.7751,  -4.9090, -16.9294],
         [-21.1737, -24.7455,  -3.2161,  -4.3623],
         ...,
         [ -6.9019,  -8.5091,  -7.4394,  -8.1909],
         [ -7.3474,  -7.2476,  -7.4733,  -7.2625],
         [ -6.9985,  -7.4603,  -7.1012,  -8.1554]],

        [[-27.5277, -11.1526,   5.1460, -38.3453],
         [ 26.7046,   6.0714, -17.9627, -17.4841],
         [-17.9027, -26.0478,  -4.0304,  -5.2121],
         ...,
         [ -6.9205,  -8.5235,  -7.4602,  -8.2067],
         [ -7.3675,  -7.2678,  -7.4983,  -7.2863],
         [ -7.0245,  -7.4780,  -7.1228,  -8.1733]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:tensor([[3, 7, 8, 1],
        [4, 3, 8, 9]], device='cuda:0')
DEBUG:root:dob emb size for input: torch.Size([3, 2, 4, 6])
WARNING:root:Implement use of query emb!
INFO:root:seq len: 4, batch: 2, docs: 3, h: 6
INFO:root:torch.Size([4, 6, 6])
INFO:root:torch.Size([4, 2, 6])
DEBUG:root:dob emb size: torch.Size([4, 6, 6])
DEBUG:root:dob emb size after pos: torch.Size([4, 6, 6])
INFO:root:Contiguous: True
INFO:root:passing input through enc
INFO:root:cuda:0
DEBUG:root:layer type: local
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([4, 6, 6]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
DEBUG:root:local doc emb size after global enc torch.Size([4, 6, 6]):
DEBUG:root:global doc emb size after global_enc torch.Size([3, 2, 6]):
DEBUG:root:global doc emb size after encoder torch.Size([3, 2, 6]):
DEBUG:root:local doc emb size after encoder torch.Size([4, 6, 6]):
INFO:root:Topk!
INFO:root:tensor([[[-8.1557e+02, -9.6669e+00, -1.0809e+03, -1.3135e+03, -1.3294e+03,
          -6.1035e-05],
         [-8.6133e+02,  0.0000e+00, -7.0551e+02, -3.6651e+02, -2.9009e+02,
           0.0000e+00]],

        [[-8.1557e+02, -9.6669e+00, -1.0809e+03, -1.3135e+03, -1.3294e+03,
          -6.1035e-05],
         [-1.0697e+03, -1.1453e+01, -1.4876e+03, -1.7160e+03, -1.7190e+03,
           0.0000e+00]]], device='cuda:0', grad_fn=<TopkBackward>)
INFO:root:torch.Size([2, 2, 6])
INFO:root:tensor([[[-57.0563,  11.7647,   3.8328, -59.5408],
         [-13.2858, -52.6226, -29.5115,  14.6826],
         [  1.6036, -23.2388,  35.1371,  34.4353],
         ...,
         [ 33.3955,  28.5810,  32.7830,  32.5972],
         [ 33.4038,  20.2292,  32.6952,  32.8452],
         [ 33.3166,  18.4378,  33.1350,  31.9542]],

        [[-52.8234,   8.9948,   5.5596, -56.4548],
         [-15.3197, -51.8782, -33.2792,  19.4265],
         [  9.7857, -18.0035,  35.5387,  34.9267],
         ...,
         [ 37.3201,  32.8599,  36.6850,  36.6988],
         [ 37.4088,  25.5540,  36.6102,  36.6726],
         [ 37.2134,  23.6316,  37.0843,  35.9636]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:tensor([[0, 1, 3, 0],
        [3, 7, 8, 1]], device='cuda:0')
DEBUG:root:dob emb size for input: torch.Size([3, 2, 4, 6])
WARNING:root:Implement use of query emb!
INFO:root:seq len: 4, batch: 2, docs: 3, h: 6
INFO:root:torch.Size([4, 6, 6])
INFO:root:torch.Size([4, 2, 6])
DEBUG:root:dob emb size: torch.Size([4, 6, 6])
DEBUG:root:dob emb size after pos: torch.Size([4, 6, 6])
INFO:root:Contiguous: True
INFO:root:passing input through enc
INFO:root:cuda:0
DEBUG:root:layer type: local
DEBUG:root:layer type: global
DEBUG:root:local doc emb size after reshaping torch.Size([4, 6, 6]):
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
INFO:root:Using global transformer
INFO:root:input_size torch.Size([4, 6, 6])
INFO:root:torch.Size([6, 6])
INFO:root:Pooling size torch.Size([3, 2, 6])
INFO:root:doc attn size torch.Size([3, 2, 6])
INFO:root:repeated doc attn size torch.Size([24, 6])
INFO:root:norm1 src torch.Size([4, 6, 6])
INFO:root:src2 linr attn torch.Size([4, 6, 6])
INFO:root:src norm attn torch.Size([4, 6, 6])
DEBUG:root:local doc emb size after global enc torch.Size([4, 6, 6]):
DEBUG:root:global doc emb size after global_enc torch.Size([3, 2, 6]):
DEBUG:root:global doc emb size after encoder torch.Size([3, 2, 6]):
DEBUG:root:local doc emb size after encoder torch.Size([4, 6, 6]):
INFO:root:Topk!
INFO:root:tensor([[[-3508.2520,  -309.3403, -4132.5605, -3631.8320, -5189.9570,
              0.0000],
         [-1288.8357,  -271.2969, -2050.9609, -2425.5396, -3287.1421,
              0.0000]],

        [[-3508.2520,  -309.3403, -4132.5605, -3631.8320, -5189.9570,
              0.0000],
         [-2963.5112,  -271.2969, -3591.9995, -3022.3108, -4355.3203,
              0.0000]]], device='cuda:0', grad_fn=<TopkBackward>)
INFO:root:torch.Size([2, 2, 6])
INFO:root:tensor([[[-11.1971,  11.2411,  -0.3760, -17.0491],
         [-21.6181, -28.8254, -19.0395,  24.4780],
         [ 11.7619,  -5.3054,  20.1085,   9.9031],
         ...,
         [ 20.7058,  25.0387,  22.0396,   9.9240],
         [ 21.6220,  19.1117,  19.6227,  10.4687],
         [ 20.4474,  17.2071,  18.8864,  11.3609]],

        [[-11.3477,  11.4767,   0.1406, -16.2227],
         [-20.8116, -28.1185, -18.5510,  22.9893],
         [ 11.3731,  -5.3205,  20.0408,  10.2293],
         ...,
         [ 21.0929,  25.4020,  22.4111,  10.2514],
         [ 21.9977,  19.4822,  19.9948,  10.7983],
         [ 20.8188,  17.5859,  19.2697,  11.6943]]], device='cuda:0',
       grad_fn=<ViewBackward>)
INFO:root:tensor([[4, 3, 8, 9],
        [1, 1, 7, 9]], device='cuda:0')
